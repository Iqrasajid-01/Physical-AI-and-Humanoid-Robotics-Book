"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[4685],{2519:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"labs_and_projects/robotics-labs","title":"Robotics Labs","description":"Learning Objectives","source":"@site/docs/labs_and_projects/23-robotics-labs.md","sourceDirName":"labs_and_projects","slug":"/labs_and_projects/23-robotics-labs","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/labs_and_projects/23-robotics-labs","draft":false,"unlisted":false,"editUrl":"https://github.com/Iqrasajid-01/docs/labs_and_projects/23-robotics-labs.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"title":"Robotics Labs","sidebar_label":"23 - Robotics Labs","slug":"/labs_and_projects/23-robotics-labs"},"sidebar":"tutorialSidebar","previous":{"title":"22 - Advanced VLA Applications","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/advanced-vla-applications"},"next":{"title":"24-Project Implementation - From Concept to Deployment","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/labs_and_projects/project-implementation"}}');var a=t(4848),s=t(8453);const i={title:"Robotics Labs",sidebar_label:"23 - Robotics Labs",slug:"/labs_and_projects/23-robotics-labs"},o="Robotics Labs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Laboratory Design Principles",id:"laboratory-design-principles",level:3},{value:"Lab Components",id:"lab-components",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Assessment and Evaluation",id:"assessment-and-evaluation",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Flow Diagram",id:"flow-diagram",level:2},{value:"Code Example: Lab Exercise Framework",id:"code-example-lab-exercise-framework",level:2},{value:"Step-by-Step Practical Tutorial",id:"step-by-step-practical-tutorial",level:2},{value:"Implementing Robotics Lab Exercises",id:"implementing-robotics-lab-exercises",level:3},{value:"Summary",id:"summary",level:2},{value:"Mini-Quiz",id:"mini-quiz",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"robotics-labs",children:"Robotics Labs"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design and implement hands-on robotics laboratory exercises"}),"\n",(0,a.jsx)(n.li,{children:"Apply theoretical concepts to practical robotics experiments"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multiple robotics subsystems in laboratory settings"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate and analyze robotics system performance through experiments"}),"\n",(0,a.jsx)(n.li,{children:"Troubleshoot common robotics hardware and software issues"}),"\n",(0,a.jsx)(n.li,{children:"Document and report laboratory findings effectively"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Robotics laboratories provide essential hands-on experience where theoretical concepts meet practical implementation. These lab exercises bridge the gap between classroom learning and real-world robotics applications, allowing students and practitioners to experiment with physical robots, test algorithms, and validate theoretical models in controlled environments."}),"\n",(0,a.jsx)(n.p,{children:"Laboratory exercises in robotics typically involve multiple subsystems including perception, planning, control, and actuation. This chapter provides guidance on designing effective robotics labs that integrate concepts from ROS2, simulation, NVIDIA Isaac, and Vision-Language-Action (VLA) models, allowing learners to build comprehensive understanding through practical experience."}),"\n",(0,a.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,a.jsx)(n.h3,{id:"laboratory-design-principles",children:"Laboratory Design Principles"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Progressive Complexity"}),": Labs should start with basic concepts and gradually increase in complexity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hands-On Learning"}),": Emphasis on practical implementation rather than theoretical analysis only"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Integration"}),": Exercises that combine multiple robotics subsystems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reproducible Experiments"}),": Well-defined procedures that can be replicated"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-components",children:"Lab Components"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hardware Setup"}),": Robot platforms, sensors, and actuator systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Software Environment"}),": ROS2, simulation tools, and development frameworks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Experimental Procedures"}),": Step-by-step instructions for conducting experiments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Collection"}),": Methods for measuring and recording system performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Analysis Tools"}),": Techniques for interpreting experimental results"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical Safety"}),": Protecting humans and equipment during experiments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Operational Safety"}),": Safe robot operation protocols"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Emergency Procedures"}),": Protocols for handling unexpected situations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Equipment Protection"}),": Safeguarding expensive robotics hardware"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"assessment-and-evaluation",children:"Assessment and Evaluation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance Metrics"}),": Quantitative measures of system performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Qualitative Assessment"}),": Observational evaluation of robot behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Documentation Requirements"}),": Proper recording of experimental procedures and results"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Troubleshooting Skills"}),": Ability to diagnose and resolve common issues"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Flow Diagram",src:t(7359).A+"",width:"1386",height:"578"})}),"\n",(0,a.jsx)(n.h2,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Flow Diagram",src:t(4088).A+"",width:"1601",height:"465"})}),"\n",(0,a.jsx)(n.h2,{id:"code-example-lab-exercise-framework",children:"Code Example: Lab Exercise Framework"}),"\n",(0,a.jsx)(n.p,{children:"Here's an example framework for implementing robotics lab exercises:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String, Float64\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom cv_bridge import CvBridge\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\nimport time\nimport threading\nimport queue\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\nimport csv\n\n\nclass LabState(Enum):\n    \"\"\"States for lab execution\"\"\"\n    SETUP = \"setup\"\n    READY = \"ready\"\n    RUNNING = \"running\"\n    PAUSED = \"paused\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n\n\nclass LabExperimentType(Enum):\n    \"\"\"Types of lab experiments\"\"\"\n    BASIC_MOBILITY = \"basic_mobility\"\n    SENSOR_FUSION = \"sensor_fusion\"\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    PERCEPTION = \"perception\"\n    VLA_INTEGRATION = \"vla_integration\"\n\n\n@dataclass\nclass LabMetrics:\n    \"\"\"Metrics collected during lab experiments\"\"\"\n    timestamp: float\n    experiment_type: LabExperimentType\n    success_rate: float = 0.0\n    execution_time: float = 0.0\n    accuracy: float = 0.0\n    efficiency: float = 0.0\n    error_count: int = 0\n    data_points: List[Dict[str, Any]] = None\n\n    def __post_init__(self):\n        if self.data_points is None:\n            self.data_points = []\n\n\n@dataclass\nclass LabConfiguration:\n    \"\"\"Configuration for lab experiments\"\"\"\n    experiment_name: str\n    experiment_type: LabExperimentType\n    duration: float  # seconds\n    success_criteria: Dict[str, float]\n    safety_limits: Dict[str, float]\n    required_equipment: List[str]\n    evaluation_metrics: List[str]\n\n\nclass RoboticsLabFramework(Node):\n    \"\"\"\n    Framework for implementing robotics laboratory exercises\n    \"\"\"\n    def __init__(self):\n        super().__init__('robotics_lab_framework')\n\n        # Initialize parameters\n        self.declare_parameter('lab_experiment_type', 'basic_mobility')\n        self.declare_parameter('lab_duration', 300.0)  # 5 minutes default\n        self.declare_parameter('data_collection_rate', 10.0)  # Hz\n        self.declare_parameter('enable_visualization', True)\n        self.declare_parameter('enable_logging', True)\n\n        # Get parameters\n        experiment_type_str = self.get_parameter('lab_experiment_type').value\n        self.experiment_type = LabExperimentType(experiment_type_str)\n        self.lab_duration = self.get_parameter('lab_duration').value\n        self.data_collection_rate = self.get_parameter('data_collection_rate').value\n        self.enable_visualization = self.get_parameter('enable_visualization').value\n        self.enable_logging = self.get_parameter('enable_logging').value\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Lab state management\n        self.lab_state = LabState.SETUP\n        self.lab_start_time = None\n        self.lab_end_time = None\n        self.current_experiment: Optional[LabConfiguration] = None\n        self.metrics = LabMetrics(\n            timestamp=time.time(),\n            experiment_type=self.experiment_type\n        )\n\n        # Data collection\n        self.data_buffer = queue.Queue(maxsize=1000)\n        self.visualization_markers = MarkerArray()\n\n        # Experiment control\n        self.experiment_thread = None\n        self.experiment_running = False\n\n        # Create publishers\n        self.status_pub = self.create_publisher(String, '/lab/status', 10)\n        self.metrics_pub = self.create_publisher(Float64, '/lab/metrics', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/lab/markers', 10)\n        self.command_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Create subscribers\n        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n        self.image_sub = self.create_subscription(Image, '/camera/image_raw', self.image_callback, 10)\n\n        # Create timers\n        self.data_collection_timer = self.create_timer(\n            1.0 / self.data_collection_rate, self.collect_data)\n        self.monitoring_timer = self.create_timer(1.0, self.monitor_experiment)\n        self.visualization_timer = self.create_timer(0.1, self.publish_visualization)\n\n        # Initialize lab\n        self.setup_lab_environment()\n\n        self.get_logger().info(\n            f'Robotics Lab Framework initialized for {self.experiment_type.value}'\n        )\n\n    def setup_lab_environment(self):\n        \"\"\"\n        Setup the laboratory environment\n        \"\"\"\n        try:\n            # Configure safety limits\n            self.safety_limits = {\n                'max_velocity': 0.5,\n                'min_distance': 0.3,\n                'max_acceleration': 1.0,\n                'max_current': 10.0  # for actuators\n            }\n\n            # Configure success criteria\n            self.success_criteria = {\n                'accuracy_threshold': 0.95,\n                'completion_time': self.lab_duration * 0.8,\n                'error_limit': 3\n            }\n\n            # Initialize data structures\n            self.robot_state = {\n                'position': np.array([0.0, 0.0, 0.0]),\n                'velocity': np.array([0.0, 0.0, 0.0]),\n                'orientation': np.array([0.0, 0.0, 0.0, 1.0]),\n                'safety_status': 'normal'\n            }\n\n            self.lab_state = LabState.READY\n            self.get_logger().info('Lab environment setup completed')\n\n        except Exception as e:\n            self.get_logger().error(f'Error setting up lab environment: {e}')\n            self.lab_state = LabState.ERROR\n\n    def start_experiment(self, config: LabConfiguration):\n        \"\"\"\n        Start a new lab experiment\n        \"\"\"\n        try:\n            self.current_experiment = config\n            self.lab_start_time = time.time()\n            self.lab_end_time = self.lab_start_time + config.duration\n            self.lab_state = LabState.RUNNING\n\n            # Start experiment thread\n            self.experiment_running = True\n            self.experiment_thread = threading.Thread(\n                target=self._run_experiment, args=(config,))\n            self.experiment_thread.start()\n\n            status_msg = String()\n            status_msg.data = f'Started_experiment:{config.experiment_name}'\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().info(f'Started experiment: {config.experiment_name}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error starting experiment: {e}')\n            self.lab_state = LabState.ERROR\n\n    def stop_experiment(self):\n        \"\"\"\n        Stop the current experiment\n        \"\"\"\n        try:\n            self.experiment_running = False\n            if self.experiment_thread:\n                self.experiment_thread.join(timeout=2.0)\n\n            self.lab_state = LabState.COMPLETED\n            self.lab_end_time = time.time()\n\n            # Calculate final metrics\n            self.calculate_final_metrics()\n\n            status_msg = String()\n            status_msg.data = f'Experiment completed. Success rate: {self.metrics.success_rate:.2f}'\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().info('Experiment stopped and metrics calculated')\n\n        except Exception as e:\n            self.get_logger().error(f'Error stopping experiment: {e}')\n\n    def _run_experiment(self, config: LabConfiguration):\n        \"\"\"\n        Run the experiment in a separate thread\n        \"\"\"\n        try:\n            if config.experiment_type == LabExperimentType.BASIC_MOBILITY:\n                self._run_mobility_experiment()\n            elif config.experiment_type == LabExperimentType.NAVIGATION:\n                self._run_navigation_experiment()\n            elif config.experiment_type == LabExperimentType.PERCEPTION:\n                self._run_perception_experiment()\n            elif config.experiment_type == LabExperimentType.VLA_INTEGRATION:\n                self._run_vla_integration_experiment()\n            # Add other experiment types as needed\n\n        except Exception as e:\n            self.get_logger().error(f'Error in experiment execution: {e}')\n            self.lab_state = LabState.ERROR\n\n    def _run_mobility_experiment(self):\n        \"\"\"\n        Run basic mobility experiment\n        \"\"\"\n        # Example: Move robot in a square pattern\n        waypoints = [\n            (1.0, 0.0),   # Move 1m forward\n            (1.0, 1.0),   # Move 1m right\n            (0.0, 1.0),   # Move 1m back\n            (0.0, 0.0)    # Move 1m left (return to start)\n        ]\n\n        for i, (target_x, target_y) in enumerate(waypoints):\n            if not self.experiment_running:\n                break\n\n            self.get_logger().info(f'Moving to waypoint {i+1}: ({target_x}, {target_y})')\n\n            # Simple proportional controller\n            while self.experiment_running:\n                current_pos = self.robot_state['position']\n                dx = target_x - current_pos[0]\n                dy = target_y - current_pos[1]\n                distance = np.sqrt(dx**2 + dy**2)\n\n                if distance < 0.1:  # Within 10cm of target\n                    break\n\n                # Calculate velocity command\n                cmd_vel = Twist()\n                cmd_vel.linear.x = min(0.3, distance * 0.5)  # Proportional control\n                cmd_vel.angular.z = np.arctan2(dy, dx) * 0.5\n\n                # Check safety\n                if self._check_safety():\n                    self.command_pub.publish(cmd_vel)\n                else:\n                    self.get_logger().warn('Safety limit exceeded, stopping')\n                    break\n\n                time.sleep(0.1)\n\n    def _run_navigation_experiment(self):\n        \"\"\"\n        Run navigation experiment\n        \"\"\"\n        # Example: Navigate to random points while avoiding obstacles\n        for _ in range(5):  # 5 random goals\n            if not self.experiment_running:\n                break\n\n            # Generate random goal\n            goal_x = np.random.uniform(-2.0, 2.0)\n            goal_y = np.random.uniform(-2.0, 2.0)\n\n            self.get_logger().info(f'Navigating to goal: ({goal_x}, {goal_y})')\n\n            # Simple navigation to goal\n            while self.experiment_running:\n                current_pos = self.robot_state['position']\n                dx = goal_x - current_pos[0]\n                dy = goal_y - current_pos[1]\n                distance = np.sqrt(dx**2 + dy**2)\n\n                if distance < 0.2:  # Within 20cm of goal\n                    break\n\n                # Calculate velocity command\n                cmd_vel = Twist()\n                cmd_vel.linear.x = min(0.4, distance * 0.8)\n                cmd_vel.angular.z = np.arctan2(dy, dx) * 0.8\n\n                # Check for obstacles\n                if hasattr(self, 'obstacle_distance') and self.obstacle_distance < 0.5:\n                    cmd_vel.linear.x = 0.0  # Stop if obstacle too close\n                    cmd_vel.angular.z *= 0.5  # Turn to avoid\n\n                # Check safety\n                if self._check_safety():\n                    self.command_pub.publish(cmd_vel)\n                else:\n                    self.get_logger().warn('Safety limit exceeded during navigation')\n                    break\n\n                time.sleep(0.1)\n\n    def _run_perception_experiment(self):\n        \"\"\"\n        Run perception experiment\n        \"\"\"\n        # Example: Detect and track objects\n        detection_count = 0\n        tracking_accuracy = 0.0\n\n        start_time = time.time()\n        while self.experiment_running and (time.time() - start_time) < 30:  # 30 second experiment\n            # Simulate object detection\n            if hasattr(self, 'latest_image'):\n                # In a real implementation, this would run object detection\n                # For simulation, we'll count detections\n                detection_count += 1\n                tracking_accuracy += np.random.uniform(0.8, 1.0)  # Simulated accuracy\n\n            time.sleep(0.5)  # Process every 0.5 seconds\n\n        self.get_logger().info(f'Perception experiment: {detection_count} detections')\n\n    def _run_vla_integration_experiment(self):\n        \"\"\"\n        Run VLA integration experiment\n        \"\"\"\n        # Example: Process voice commands and execute actions\n        for _ in range(3):  # 3 command cycles\n            if not self.experiment_running:\n                break\n\n            # Simulate voice command processing\n            commands = [\"move forward\", \"turn left\", \"grasp object\"]\n            command = np.random.choice(commands)\n\n            self.get_logger().info(f'Processing command: {command}')\n\n            # Execute based on command\n            cmd_vel = Twist()\n            if \"move\" in command:\n                cmd_vel.linear.x = 0.3\n            elif \"turn\" in command:\n                cmd_vel.angular.z = 0.5\n            # In a real system, this would integrate with VLA components\n\n            self.command_pub.publish(cmd_vel)\n            time.sleep(2.0)  # Execute for 2 seconds\n\n    def odom_callback(self, msg: Odometry):\n        \"\"\"\n        Handle odometry updates\n        \"\"\"\n        try:\n            self.robot_state['position'] = np.array([\n                msg.pose.pose.position.x,\n                msg.pose.pose.position.y,\n                msg.pose.pose.position.z\n            ])\n\n            # Convert quaternion to euler for orientation\n            import math\n            from tf_transformations import euler_from_quaternion\n            orientation = msg.pose.pose.orientation\n            roll, pitch, yaw = euler_from_quaternion([\n                orientation.x, orientation.y, orientation.z, orientation.w\n            ])\n            self.robot_state['orientation'] = np.array([roll, pitch, yaw])\n\n            self.robot_state['velocity'] = np.array([\n                msg.twist.twist.linear.x,\n                msg.twist.twist.linear.y,\n                msg.twist.twist.angular.z\n            ])\n\n        except Exception as e:\n            self.get_logger().error(f'Error in odom callback: {e}')\n\n    def scan_callback(self, msg: LaserScan):\n        \"\"\"\n        Handle laser scan updates\n        \"\"\"\n        try:\n            # Process laser scan for obstacle detection\n            ranges = np.array(msg.ranges)\n            valid_ranges = ranges[np.isfinite(ranges)]  # Remove invalid readings\n\n            if len(valid_ranges) > 0:\n                self.obstacle_distance = np.min(valid_ranges)\n                self.robot_state['safety_status'] = 'normal' if self.obstacle_distance > 0.3 else 'warning'\n\n                # Update safety state\n                if self.obstacle_distance < 0.2:\n                    self.robot_state['safety_status'] = 'critical'\n                    if self.lab_state == LabState.RUNNING:\n                        self.get_logger().warn('Obstacle too close, safety action required')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in scan callback: {e}')\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"\n        Handle IMU updates\n        \"\"\"\n        try:\n            # Process IMU data for stability and safety\n            linear_accel = np.sqrt(\n                msg.linear_acceleration.x**2 +\n                msg.linear_acceleration.y**2 +\n                msg.linear_acceleration.z**2\n            )\n\n            if linear_accel > 5.0:  # High acceleration detected\n                self.robot_state['safety_status'] = 'warning'\n                self.get_logger().warn(f'High acceleration detected: {linear_accel:.2f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in IMU callback: {e}')\n\n    def image_callback(self, msg: Image):\n        \"\"\"\n        Handle image updates\n        \"\"\"\n        try:\n            # Store latest image for processing\n            self.latest_image = msg\n\n            # In a real implementation, this would process the image\n            # for perception experiments\n\n        except Exception as e:\n            self.get_logger().error(f'Error in image callback: {e}')\n\n    def collect_data(self):\n        \"\"\"\n        Collect data for metrics calculation\n        \"\"\"\n        if self.lab_state != LabState.RUNNING:\n            return\n\n        try:\n            current_time = time.time()\n            if self.lab_start_time:\n                elapsed_time = current_time - self.lab_start_time\n\n                # Collect relevant data points\n                data_point = {\n                    'timestamp': current_time,\n                    'elapsed_time': elapsed_time,\n                    'position': self.robot_state['position'].tolist(),\n                    'velocity': self.robot_state['velocity'].tolist(),\n                    'safety_status': self.robot_state['safety_status'],\n                    'obstacle_distance': getattr(self, 'obstacle_distance', float('inf'))\n                }\n\n                if not self.data_buffer.full():\n                    self.data_buffer.put(data_point)\n                else:\n                    # Remove oldest if buffer full\n                    try:\n                        self.data_buffer.get_nowait()\n                        self.data_buffer.put(data_point)\n                    except queue.Empty:\n                        pass\n\n        except Exception as e:\n            self.get_logger().error(f'Error in data collection: {e}')\n\n    def calculate_metrics(self) -> LabMetrics:\n        \"\"\"\n        Calculate current lab metrics\n        \"\"\"\n        try:\n            # Calculate from collected data\n            data_points = []\n            temp_buffer = []\n\n            # Get all data points from buffer\n            while not self.data_buffer.empty():\n                try:\n                    data_points.append(self.data_buffer.get_nowait())\n                except queue.Empty:\n                    break\n\n            # Put data back in buffer\n            for dp in data_points:\n                if not self.data_buffer.full():\n                    self.data_buffer.put(dp)\n                temp_buffer.append(dp)\n\n            if not temp_buffer:\n                return self.metrics\n\n            # Calculate metrics\n            positions = np.array([dp['position'] for dp in temp_buffer])\n            velocities = np.array([dp['velocity'] for dp in temp_buffer])\n\n            # Success rate: percentage of time robot was in safe state\n            safe_states = [dp for dp in temp_buffer if dp['safety_status'] == 'normal']\n            success_rate = len(safe_states) / len(temp_buffer) if temp_buffer else 0.0\n\n            # Accuracy: how close to desired positions (for mobility experiments)\n            if len(positions) > 1:\n                avg_velocity = np.mean(np.linalg.norm(velocities, axis=1))\n                efficiency = avg_velocity / self.safety_limits['max_velocity']\n\n                self.metrics = LabMetrics(\n                    timestamp=time.time(),\n                    experiment_type=self.experiment_type,\n                    success_rate=success_rate,\n                    execution_time=len(temp_buffer) / self.data_collection_rate,\n                    accuracy=0.0,  # Would be calculated based on specific experiment\n                    efficiency=efficiency,\n                    error_count=len([dp for dp in temp_buffer if dp['safety_status'] == 'critical']),\n                    data_points=temp_buffer\n                )\n\n            # Publish metrics\n            metrics_msg = Float64()\n            metrics_msg.data = float(self.metrics.success_rate)\n            self.metrics_pub.publish(metrics_msg)\n\n            return self.metrics\n\n        except Exception as e:\n            self.get_logger().error(f'Error calculating metrics: {e}')\n            return self.metrics\n\n    def calculate_final_metrics(self):\n        \"\"\"\n        Calculate final metrics at experiment completion\n        \"\"\"\n        try:\n            # Get all remaining data\n            all_data = []\n            while not self.data_buffer.empty():\n                try:\n                    all_data.append(self.data_buffer.get_nowait())\n                except queue.Empty:\n                    break\n\n            if not all_data:\n                return\n\n            # Calculate comprehensive metrics\n            total_time = self.lab_end_time - self.lab_start_time if self.lab_start_time else 0\n            success_states = [d for d in all_data if d['safety_status'] == 'normal']\n\n            final_success_rate = len(success_states) / len(all_data) if all_data else 0.0\n\n            # Update metrics\n            self.metrics.success_rate = final_success_rate\n            self.metrics.execution_time = total_time\n\n            # Log results\n            self.get_logger().info(\n                f'Final Metrics - Success Rate: {final_success_rate:.2f}, '\n                f'Execution Time: {total_time:.2f}s, '\n                f'Errors: {self.metrics.error_count}'\n            )\n\n            # Save to file\n            self._save_experiment_results()\n\n        except Exception as e:\n            self.get_logger().error(f'Error calculating final metrics: {e}')\n\n    def _save_experiment_results(self):\n        \"\"\"\n        Save experiment results to file\n        \"\"\"\n        try:\n            filename = f\"lab_results_{self.experiment_type.value}_{int(time.time())}.json\"\n\n            results = {\n                'experiment_type': self.experiment_type.value,\n                'start_time': self.lab_start_time,\n                'end_time': self.lab_end_time,\n                'duration': self.metrics.execution_time,\n                'success_rate': self.metrics.success_rate,\n                'error_count': self.metrics.error_count,\n                'efficiency': self.metrics.efficiency,\n                'data_points_count': len(self.metrics.data_points)\n            }\n\n            with open(filename, 'w') as f:\n                json.dump(results, f, indent=2)\n\n            self.get_logger().info(f'Experiment results saved to {filename}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error saving results: {e}')\n\n    def _check_safety(self) -> bool:\n        \"\"\"\n        Check if current robot state is within safety limits\n        \"\"\"\n        try:\n            # Check velocity limits\n            vel_magnitude = np.linalg.norm(self.robot_state['velocity'])\n            if vel_magnitude > self.safety_limits['max_velocity']:\n                return False\n\n            # Check obstacle distance (if available)\n            if hasattr(self, 'obstacle_distance'):\n                if self.obstacle_distance < self.safety_limits['min_distance']:\n                    return False\n\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error in safety check: {e}')\n            return False\n\n    def monitor_experiment(self):\n        \"\"\"\n        Monitor experiment status and safety\n        \"\"\"\n        try:\n            # Check if experiment should end\n            if (self.lab_state == LabState.RUNNING and\n                self.lab_end_time and\n                time.time() > self.lab_end_time):\n                self.stop_experiment()\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f'State: {self.lab_state.value}, ' \\\n                             f'Experiment: {self.current_experiment.experiment_name if self.current_experiment else \"None\"}, ' \\\n                             f'Running: {self.experiment_running}'\n            self.status_pub.publish(status_msg)\n\n            # Calculate and log metrics periodically\n            if self.lab_state == LabState.RUNNING:\n                metrics = self.calculate_metrics()\n                self.get_logger().debug(\n                    f'Current metrics - Success: {metrics.success_rate:.2f}, '\n                    f'Errors: {metrics.error_count}'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f'Error in monitoring: {e}')\n\n    def publish_visualization(self):\n        \"\"\"\n        Publish visualization markers for lab environment\n        \"\"\"\n        if not self.enable_visualization:\n            return\n\n        try:\n            marker_array = MarkerArray()\n            current_time = self.get_clock().now()\n\n            # Create markers for robot path\n            if hasattr(self, 'robot_state'):\n                path_marker = Marker()\n                path_marker.header.frame_id = 'map'\n                path_marker.header.stamp = current_time.to_msg()\n                path_marker.ns = 'robot_path'\n                path_marker.id = 0\n                path_marker.type = Marker.SPHERE\n                path_marker.action = Marker.ADD\n\n                path_marker.pose.position.x = float(self.robot_state['position'][0])\n                path_marker.pose.position.y = float(self.robot_state['position'][1])\n                path_marker.pose.position.z = 0.0\n                path_marker.pose.orientation.w = 1.0\n\n                path_marker.scale.x = 0.1\n                path_marker.scale.y = 0.1\n                path_marker.scale.z = 0.1\n\n                path_marker.color.r = 1.0\n                path_marker.color.g = 0.0\n                path_marker.color.b = 0.0\n                path_marker.color.a = 0.8\n\n                marker_array.markers.append(path_marker)\n\n            self.marker_pub.publish(marker_array)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in visualization: {e}')\n\n    def destroy_node(self):\n        \"\"\"\n        Clean up resources when node is destroyed\n        \"\"\"\n        self.get_logger().info('Cleaning up Robotics Lab Framework')\n        if self.experiment_running:\n            self.stop_experiment()\n        super().destroy_node()\n\n\nclass LabExperimentManager:\n    \"\"\"\n    Manager for organizing and running lab experiments\n    \"\"\"\n    def __init__(self):\n        self.available_experiments = {}\n        self.completed_experiments = []\n        self.current_experiment = None\n\n    def register_experiment(self, name: str, config: LabConfiguration):\n        \"\"\"\n        Register a new experiment configuration\n        \"\"\"\n        self.available_experiments[name] = config\n        print(f\"Registered experiment: {name}\")\n\n    def get_experiment(self, name: str) -> Optional[LabConfiguration]:\n        \"\"\"\n        Get a registered experiment configuration\n        \"\"\"\n        return self.available_experiments.get(name)\n\n    def run_experiment(self, framework: RoboticsLabFramework, name: str) -> bool:\n        \"\"\"\n        Run a registered experiment\n        \"\"\"\n        config = self.get_experiment(name)\n        if not config:\n            print(f\"Experiment {name} not found\")\n            return False\n\n        try:\n            framework.start_experiment(config)\n            self.current_experiment = config\n            return True\n        except Exception as e:\n            print(f\"Error running experiment {name}: {e}\")\n            return False\n\n    def create_basic_mobility_config(self) -> LabConfiguration:\n        \"\"\"\n        Create configuration for basic mobility lab\n        \"\"\"\n        return LabConfiguration(\n            experiment_name=\"Basic Mobility Test\",\n            experiment_type=LabExperimentType.BASIC_MOBILITY,\n            duration=120.0,  # 2 minutes\n            success_criteria={\n                'accuracy_threshold': 0.90,\n                'completion_time': 100.0,\n                'max_errors': 2\n            },\n            safety_limits={\n                'max_velocity': 0.5,\n                'min_distance': 0.3\n            },\n            required_equipment=['robot_base', 'laser_scanner', 'odometry'],\n            evaluation_metrics=['success_rate', 'accuracy', 'efficiency']\n        )\n\n    def create_navigation_config(self) -> LabConfiguration:\n        \"\"\"\n        Create configuration for navigation lab\n        \"\"\"\n        return LabConfiguration(\n            experiment_name=\"Navigation Test\",\n            experiment_type=LabExperimentType.NAVIGATION,\n            duration=300.0,  # 5 minutes\n            success_criteria={\n                'accuracy_threshold': 0.85,\n                'completion_time': 240.0,\n                'max_errors': 5\n            },\n            safety_limits={\n                'max_velocity': 0.4,\n                'min_distance': 0.4\n            },\n            required_equipment=['robot_base', 'laser_scanner', 'odometry', 'mapping_system'],\n            evaluation_metrics=['success_rate', 'path_efficiency', 'obstacle_avoidance']\n        )\n\n    def create_perception_config(self) -> LabConfiguration:\n        \"\"\"\n        Create configuration for perception lab\n        \"\"\"\n        return LabConfiguration(\n            experiment_name=\"Perception Test\",\n            experiment_type=LabExperimentType.PERCEPTION,\n            duration=180.0,  # 3 minutes\n            success_criteria={\n                'detection_accuracy': 0.80,\n                'tracking_stability': 0.90,\n                'max_errors': 3\n            },\n            safety_limits={\n                'max_velocity': 0.2,  # Slow for perception\n                'min_distance': 0.5\n            },\n            required_equipment=['camera', 'robot_base', 'lighting'],\n            evaluation_metrics=['detection_rate', 'accuracy', 'processing_time']\n        )\n\n\ndef create_lab_curriculum():\n    \"\"\"\n    Create a complete lab curriculum\n    \"\"\"\n    curriculum = {\n        'level_1': {\n            'name': 'Basic Robotics Concepts',\n            'experiments': [\n                'Motor Control',\n                'Basic Sensing',\n                'Simple Navigation'\n            ],\n            'duration_hours': 6,\n            'prerequisites': ['Basic programming', 'Introduction to ROS']\n        },\n        'level_2': {\n            'name': 'Intermediate Robotics',\n            'experiments': [\n                'Sensor Fusion',\n                'Path Planning',\n                'Object Manipulation'\n            ],\n            'duration_hours': 12,\n            'prerequisites': ['Level 1 completion', 'Basic control theory']\n        },\n        'level_3': {\n            'name': 'Advanced Robotics Applications',\n            'experiments': [\n                'SLAM Implementation',\n                'Vision-Based Control',\n                'Human-Robot Interaction'\n            ],\n            'duration_hours': 18,\n            'prerequisites': ['Level 2 completion', 'Advanced programming']\n        }\n    }\n\n    return curriculum\n\n\ndef main(args=None):\n    \"\"\"\n    Main function for robotics lab framework\n    \"\"\"\n    rclpy.init(args=args)\n\n    try:\n        # Create lab framework\n        lab_framework = RoboticsLabFramework()\n\n        # Create experiment manager\n        exp_manager = LabExperimentManager()\n\n        # Register experiments\n        exp_manager.register_experiment('basic_mobility', exp_manager.create_basic_mobility_config())\n        exp_manager.register_experiment('navigation', exp_manager.create_navigation_config())\n        exp_manager.register_experiment('perception', exp_manager.create_perception_config())\n\n        # Example: Run basic mobility experiment\n        exp_manager.run_experiment(lab_framework, 'basic_mobility')\n\n        # Spin the framework\n        rclpy.spin(lab_framework)\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        if 'lab_framework' in locals():\n            lab_framework.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-by-step-practical-tutorial",children:"Step-by-Step Practical Tutorial"}),"\n",(0,a.jsx)(n.h3,{id:"implementing-robotics-lab-exercises",children:"Implementing Robotics Lab Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a lab package"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python robotics_labs_examples --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs visualization_msgs cv_bridge tf2_ros\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Navigate to the package directory"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd robotics_labs_examples\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create the main module directory"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir robotics_labs_examples\ntouch robotics_labs_examples/__init__.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create the lab framework implementation"})," (",(0,a.jsx)(n.code,{children:"robotics_labs_examples/lab_framework.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Use the lab framework code example above\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a configuration file"})," (",(0,a.jsx)(n.code,{children:"config/lab_config.yaml"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'robotics_lab_framework:\n  ros__parameters:\n    # Lab parameters\n    lab_experiment_type: "basic_mobility"\n    lab_duration: 300.0\n    data_collection_rate: 10.0\n    enable_visualization: true\n    enable_logging: true\n\n    # Safety parameters\n    max_velocity: 0.5\n    min_distance: 0.3\n    max_acceleration: 1.0\n\n    # Evaluation parameters\n    success_threshold: 0.8\n    error_limit: 5\n\n    # Topic configuration\n    odom_topic: "/odom"\n    scan_topic: "/scan"\n    image_topic: "/camera/image_raw"\n    cmd_vel_topic: "/cmd_vel"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create lab exercise files"})," (",(0,a.jsx)(n.code,{children:"robotics_labs_examples/lab_exercises/"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir robotics_labs_examples/lab_exercises\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a basic mobility lab"})," (",(0,a.jsx)(n.code,{children:"robotics_labs_examples/lab_exercises/basic_mobility.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'"""\nBasic Mobility Lab Exercise\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\nimport time\n\n\nclass BasicMobilityLab(Node):\n    """\n    Basic mobility lab exercise: Move robot in geometric patterns\n    """\n    def __init__(self):\n        super().__init__(\'basic_mobility_lab\')\n\n        # Robot state\n        self.current_position = np.array([0.0, 0.0])\n        self.current_orientation = 0.0\n        self.safety_distance = 0.5\n\n        # Create publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.odom_sub = self.create_subscription(Odometry, \'/odom\', self.odom_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n\n        # Create timer for control loop\n        self.control_timer = self.create_timer(0.1, self.control_loop)\n\n        self.get_logger().info(\'Basic Mobility Lab initialized\')\n\n    def odom_callback(self, msg):\n        """\n        Handle odometry updates\n        """\n        self.current_position[0] = msg.pose.pose.position.x\n        self.current_position[1] = msg.pose.pose.position.y\n\n    def scan_callback(self, msg):\n        """\n        Handle laser scan for obstacle detection\n        """\n        if len(msg.ranges) > 0:\n            min_distance = min([r for r in msg.ranges if r > 0])\n            if min_distance < self.safety_distance:\n                self.get_logger().warn(f\'Obstacle detected: {min_distance:.2f}m\')\n\n    def control_loop(self):\n        """\n        Main control loop for mobility exercise\n        """\n        # Example: Move in a square pattern\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.3  # Move forward at 0.3 m/s\n        cmd_vel.angular.z = 0.0\n\n        # In a real exercise, this would implement the specific mobility task\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def execute_square_pattern(self):\n        """\n        Execute movement in a square pattern\n        """\n        waypoints = [\n            (1.0, 0.0),   # Move 1m forward\n            (1.0, 1.0),   # Move 1m right\n            (0.0, 1.0),   # Move 1m back\n            (0.0, 0.0)    # Move 1m left\n        ]\n\n        for target_x, target_y in waypoints:\n            self.move_to_position(target_x, target_y)\n\n    def move_to_position(self, target_x, target_y):\n        """\n        Move robot to target position\n        """\n        while True:\n            dx = target_x - self.current_position[0]\n            dy = target_y - self.current_position[1]\n            distance = np.sqrt(dx**2 + dy**2)\n\n            if distance < 0.1:  # Within 10cm\n                break\n\n            cmd_vel = Twist()\n            cmd_vel.linear.x = min(0.3, distance * 0.5)\n            cmd_vel.angular.z = np.arctan2(dy, dx) * 0.5\n\n            self.cmd_vel_pub.publish(cmd_vel)\n            time.sleep(0.1)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    lab_node = BasicMobilityLab()\n\n    try:\n        rclpy.spin(lab_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lab_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a perception lab"})," (",(0,a.jsx)(n.code,{children:"robotics_labs_examples/lab_exercises/perception_lab.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'"""\nPerception Lab Exercise\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\n\nclass PerceptionLab(Node):\n    """\n    Perception lab exercise: Object detection and tracking\n    """\n    def __init__(self):\n        super().__init__(\'perception_lab\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Create publishers and subscribers\n        self.image_sub = self.create_subscription(Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n        self.result_pub = self.create_publisher(String, \'/perception/results\', 10)\n\n        self.get_logger().info(\'Perception Lab initialized\')\n\n    def image_callback(self, msg):\n        """\n        Process camera images for object detection\n        """\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Simple color-based object detection (example)\n            hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n\n            # Define range for red color\n            lower_red = np.array([0, 100, 100])\n            upper_red = np.array([10, 255, 255])\n            mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n            lower_red = np.array([170, 100, 100])\n            upper_red = np.array([180, 255, 255])\n            mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n            mask = mask1 + mask2\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            if contours:\n                # Find largest contour\n                largest_contour = max(contours, key=cv2.contourArea)\n                if cv2.contourArea(largest_contour) > 500:  # Minimum area threshold\n                    # Calculate center of object\n                    M = cv2.moments(largest_contour)\n                    if M["m00"] != 0:\n                        cx = int(M["m10"] / M["m00"])\n                        cy = int(M["m01"] / M["m00"])\n\n                        result_msg = String()\n                        result_msg.data = f\'Red object detected at ({cx}, {cy})\'\n                        self.result_pub.publish(result_msg)\n\n                        self.get_logger().info(f\'Red object detected at ({cx}, {cy})\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def scan_callback(self, msg):\n        """\n        Process laser scan for object detection\n        """\n        # In a real implementation, this would detect objects using LIDAR\n        pass\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    lab_node = PerceptionLab()\n\n    try:\n        rclpy.spin(lab_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lab_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create launch directory"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir launch\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a launch file"})," (",(0,a.jsx)(n.code,{children:"launch/robotics_labs_example.launch.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    # Get package share directory\n    pkg_share = get_package_share_directory('robotics_labs_examples')\n    config_file = os.path.join(pkg_share, 'config', 'lab_config.yaml')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n\n        # Lab framework node\n        Node(\n            package='robotics_labs_examples',\n            executable='robotics_labs_examples.lab_framework',\n            name='robotics_lab_framework',\n            parameters=[\n                config_file,\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Basic mobility lab\n        Node(\n            package='robotics_labs_examples',\n            executable='robotics_labs_examples.lab_exercises.basic_mobility',\n            name='basic_mobility_lab',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Perception lab\n        Node(\n            package='robotics_labs_examples',\n            executable='robotics_labs_examples.lab_exercises.perception_lab',\n            name='perception_lab',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        )\n    ])\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Update setup.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'robotics_labs_examples'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name, f'{package_name}.lab_exercises'],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\n        (os.path.join('share', package_name, 'config'), glob('config/*.yaml')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='User',\n    maintainer_email='user@example.com',\n    description='Robotics lab examples for education',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'lab_framework = robotics_labs_examples.lab_framework:main',\n            'basic_mobility_lab = robotics_labs_examples.lab_exercises.basic_mobility:main',\n            'perception_lab = robotics_labs_examples.lab_exercises.perception_lab:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Build the package"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select robotics_labs_examples\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Source the workspace"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Run the robotics lab example"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 launch robotics_labs_examples robotics_labs_example.launch.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Monitor lab status"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /lab/status\nros2 topic echo /lab/metrics\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter provided a comprehensive framework for implementing robotics laboratory exercises that integrate concepts from throughout the textbook. We covered the design principles for effective lab exercises, safety considerations, and practical implementation techniques for various types of robotics experiments."}),"\n",(0,a.jsx)(n.p,{children:"Robotics labs are essential for bridging the gap between theoretical knowledge and practical application, allowing students to gain hands-on experience with real robotic systems. The framework provided enables the creation of progressive, safe, and educational laboratory experiences."}),"\n",(0,a.jsx)(n.h2,{id:"mini-quiz",children:"Mini-Quiz"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What is a key principle in designing robotics lab exercises?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A) Focus only on software"}),"\n",(0,a.jsx)(n.li,{children:"B) Progressive complexity from basic to advanced"}),"\n",(0,a.jsx)(n.li,{children:"C) Only use simulation"}),"\n",(0,a.jsx)(n.li,{children:"D) Focus on theory only"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Which safety consideration is important in robotics labs?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A) Only software safety"}),"\n",(0,a.jsx)(n.li,{children:"B) Physical safety for humans and equipment"}),"\n",(0,a.jsx)(n.li,{children:"C) Only network security"}),"\n",(0,a.jsx)(n.li,{children:"D) Only data protection"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What should be included in lab assessment?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A) Only written tests"}),"\n",(0,a.jsx)(n.li,{children:"B) Performance metrics and qualitative assessment"}),"\n",(0,a.jsx)(n.li,{children:"C) Only final results"}),"\n",(0,a.jsx)(n.li,{children:"D) Only attendance"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Why is data collection important in lab exercises?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A) Only for reports"}),"\n",(0,a.jsx)(n.li,{children:"B) To measure performance and validate theories"}),"\n",(0,a.jsx)(n.li,{children:"C) Only for documentation"}),"\n",(0,a.jsx)(n.li,{children:"D) Only for publications"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What is the purpose of lab documentation?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A) Only for instructors"}),"\n",(0,a.jsx)(n.li,{children:"B) To record procedures and results for learning"}),"\n",(0,a.jsx)(n.li,{children:"C) Only for safety"}),"\n",(0,a.jsx)(n.li,{children:"D) Only for equipment tracking"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Answers"}),": 1-B, 2-B, 3-B, 4-B, 5-B"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},4088:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/ch23-flow-d158e6752a7315d22f32a7de99e4286e.svg"},7359:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/ch23-ad-df85e497ca6e5f83a393c3053052a374.svg"},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var r=t(6540);const a={},s=r.createContext(a);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);