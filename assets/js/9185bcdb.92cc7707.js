"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[3675],{3641:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/ch21-ad-af34972e122d782ae3bd300eeb7d7810.svg"},5190:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/ch21-flow-d62443ffbf12f066ba0c138e37ba9e8f.svg"},6464:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4_vla/whisper-integration-for-voice-commands","title":"Whisper Integration for Voice Commands","description":"Learning Objectives","source":"@site/docs/module4_vla/21-whisper-integration-for-voice-commands.md","sourceDirName":"module4_vla","slug":"/module4_vla/whisper-integration-for-voice-commands","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/whisper-integration-for-voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/Iqrasajid-01/docs/module4_vla/21-whisper-integration-for-voice-commands.md","tags":[],"version":"current","sidebarPosition":21,"frontMatter":{"title":"Whisper Integration for Voice Commands","sidebar_label":"21 - Whisper Integration for Voice Commands"},"sidebar":"tutorialSidebar","previous":{"title":"20 - VLA Integration with Robotics","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/integration-with-robotics"},"next":{"title":"22 - Advanced VLA Applications","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/advanced-vla-applications"}}');var s=i(4848),o=i(8453);const r={title:"Whisper Integration for Voice Commands",sidebar_label:"21 - Whisper Integration for Voice Commands"},t="Whisper Integration for Voice Commands",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Whisper Model Architecture",id:"whisper-model-architecture",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:3},{value:"Real-time Processing Considerations",id:"real-time-processing-considerations",level:3},{value:"Voice Command Design",id:"voice-command-design",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Flow Diagram",id:"flow-diagram",level:2},{value:"Code Example: Whisper Integration for Voice Commands",id:"code-example-whisper-integration-for-voice-commands",level:2},{value:"Advanced Whisper Integration Example",id:"advanced-whisper-integration-example",level:2},{value:"Step-by-Step Practical Tutorial",id:"step-by-step-practical-tutorial",level:2},{value:"Implementing Whisper Voice Command Integration",id:"implementing-whisper-voice-command-integration",level:3},{value:"Summary",id:"summary",level:2},{value:"Mini-Quiz",id:"mini-quiz",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"whisper-integration-for-voice-commands",children:"Whisper Integration for Voice Commands"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate OpenAI Whisper for speech recognition in robotic systems"}),"\n",(0,s.jsx)(n.li,{children:"Implement real-time voice command processing pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Design voice command grammars and recognition strategies"}),"\n",(0,s.jsx)(n.li,{children:"Optimize Whisper models for robotic applications"}),"\n",(0,s.jsx)(n.li,{children:"Handle voice command ambiguities and error recovery"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate voice command recognition performance in robotics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Voice command integration enables robots to understand and respond to natural spoken language, providing a more intuitive and accessible interaction paradigm. OpenAI's Whisper model has revolutionized speech recognition with its robust performance across different accents, languages, and acoustic conditions. This chapter explores the integration of Whisper with robotic systems to enable voice-controlled robot operation."}),"\n",(0,s.jsx)(n.p,{children:"Whisper's ability to handle diverse audio conditions makes it particularly valuable for robotics applications where acoustic environments can vary significantly. The integration involves real-time audio processing, speech-to-text conversion, natural language understanding, and command execution."}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"whisper-model-architecture",children:"Whisper Model Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is built on a Transformer architecture with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoder"}),": Processes audio spectrograms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoder"}),": Generates text transcriptions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Capability"}),": Supports multiple languages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Handles various acoustic conditions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The typical pipeline includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Capture"}),": Recording voice commands from microphones"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Noise reduction and audio enhancement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Converting speech to text with Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting command meaning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Execution"}),": Translating commands to robot actions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-time-processing-considerations",children:"Real-time Processing Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency"}),": Minimizing delay between speech and action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Throughput"}),": Processing audio streams in real-time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Usage"}),": Efficient use of computational resources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reliability"}),": Consistent performance in varying conditions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"voice-command-design",children:"Voice Command Design"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grammar Structure"}),": Defining acceptable command formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vocabulary Limitations"}),": Managing recognition accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Handling unclear commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback Mechanisms"}),": Confirming command understanding"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Flow Diagram",src:i(3641).A+"",width:"731",height:"1394"})}),"\n",(0,s.jsx)(n.h2,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Flow Diagram",src:i(5190).A+"",width:"1294",height:"651"})}),"\n",(0,s.jsx)(n.h2,{id:"code-example-whisper-integration-for-voice-commands",children:"Code Example: Whisper Integration for Voice Commands"}),"\n",(0,s.jsx)(n.p,{children:"Here's an example implementation of Whisper integration with a robotic system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Header\nimport numpy as np\nimport torch\nimport whisper\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport time\nimport re\nfrom typing import Optional, Dict, Any, List\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Represents a recognized voice command\"\"\"\n    text: str\n    confidence: float\n    timestamp: float\n    interpreted_action: Optional[str] = None\n    parameters: Optional[Dict[str, Any]] = None\n\n\nclass WhisperVoiceCommandNode(Node):\n    \"\"\"\n    Node that integrates Whisper for voice command recognition\n    \"\"\"\n    def __init__(self):\n        super().__init__('whisper_voice_command_node')\n\n        # Initialize parameters\n        self.declare_parameter('model_size', 'base')  # tiny, base, small, medium, large\n        self.declare_parameter('processing_rate', 2.0)  # Process every 2 seconds\n        self.declare_parameter('enable_gpu_processing', True)\n        self.declare_parameter('audio_chunk_size', 1024)\n        self.declare_parameter('sample_rate', 16000)\n        self.declare_parameter('command_timeout', 5.0)\n        self.declare_parameter('min_confidence', 0.7)\n\n        # Get parameters\n        self.model_size = self.get_parameter('model_size').value\n        self.processing_rate = self.get_parameter('processing_rate').value\n        self.enable_gpu_processing = self.get_parameter('enable_gpu_processing').value\n        self.audio_chunk_size = self.get_parameter('audio_chunk_size').value\n        self.sample_rate = self.get_parameter('sample_rate').value\n        self.command_timeout = self.get_parameter('command_timeout').value\n        self.min_confidence = self.get_parameter('min_confidence').value\n\n        # Initialize Whisper model\n        self.whisper_model = None\n        self._initialize_whisper_model()\n\n        # Audio processing\n        self.audio_buffer = queue.Queue(maxsize=100)  # Store audio chunks\n        self.recording = False\n        self.audio_lock = threading.Lock()\n\n        # Command processing\n        self.command_queue = queue.Queue(maxsize=10)\n        self.active_command: Optional[VoiceCommand] = None\n        self.command_timeout_timer = None\n\n        # Create publishers\n        self.command_pub = self.create_publisher(String, '/robot/command', 10)\n        self.status_pub = self.create_publisher(String, '/whisper/status', 10)\n        self.feedback_pub = self.create_publisher(String, '/whisper/feedback', 10)\n\n        # Create subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/microphone/audio_raw', self.audio_callback, 10)\n        self.wake_word_sub = self.create_subscription(\n            String, '/whisper/wake_word', self.wake_word_callback, 10)\n\n        # Create timers\n        self.processing_timer = self.create_timer(\n            1.0 / self.processing_rate, self.process_audio)\n        self.monitoring_timer = self.create_timer(1.0, self.monitor_system)\n\n        # Processing statistics\n        self.processed_commands = 0\n        self.recognition_errors = 0\n\n        self.get_logger().info(\n            f'Whisper Voice Command Node initialized with {self.model_size} model'\n        )\n\n    def _initialize_whisper_model(self):\n        \"\"\"\n        Initialize the Whisper model\n        \"\"\"\n        try:\n            # Determine device\n            device = 'cuda' if self.enable_gpu_processing and torch.cuda.is_available() else 'cpu'\n            self.get_logger().info(f'Loading Whisper {self.model_size} model on {device}')\n\n            # Load model\n            self.whisper_model = whisper.load_model(self.model_size).to(device)\n\n            # Test model availability\n            dummy_audio = np.zeros(int(self.sample_rate * 1.0))  # 1 second of silence\n            result = self.whisper_model.transcribe(dummy_audio)\n            self.get_logger().info('Whisper model loaded successfully')\n\n        except Exception as e:\n            self.get_logger().error(f'Failed to load Whisper model: {e}')\n            self.whisper_model = None\n\n    def audio_callback(self, msg: AudioData):\n        \"\"\"\n        Handle incoming audio data\n        \"\"\"\n        try:\n            # Convert audio data to numpy array\n            # Assuming the audio data is in int16 format\n            audio_array = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to audio buffer\n            if not self.audio_buffer.full():\n                self.audio_buffer.put({\n                    'audio': audio_array,\n                    'timestamp': time.time(),\n                    'header': msg.header\n                })\n                self.get_logger().debug(f'Audio chunk added to buffer, size: {self.audio_buffer.qsize()}')\n            else:\n                # Buffer full, drop oldest\n                try:\n                    self.audio_buffer.get_nowait()\n                    self.audio_buffer.put({\n                        'audio': audio_array,\n                        'timestamp': time.time(),\n                        'header': msg.header\n                    })\n                except queue.Empty:\n                    pass\n\n        except Exception as e:\n            self.get_logger().error(f'Error in audio callback: {e}')\n\n    def wake_word_callback(self, msg: String):\n        \"\"\"\n        Handle wake word detection\n        \"\"\"\n        try:\n            wake_word = msg.data.lower()\n            self.get_logger().info(f'Wake word detected: {wake_word}')\n\n            # Start recording for command\n            self.start_recording_command()\n\n        except Exception as e:\n            self.get_logger().error(f'Error in wake word callback: {e}')\n\n    def start_recording_command(self):\n        \"\"\"\n        Start recording a voice command\n        \"\"\"\n        try:\n            self.recording = True\n            self.get_logger().info('Started recording voice command')\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = 'recording_voice_command'\n            self.status_pub.publish(status_msg)\n\n            # Set timeout for command\n            if self.command_timeout_timer:\n                self.command_timeout_timer.cancel()\n\n            self.command_timeout_timer = self.create_timer(\n                self.command_timeout, self.command_timeout_callback)\n\n        except Exception as e:\n            self.get_logger().error(f'Error starting recording: {e}')\n\n    def command_timeout_callback(self):\n        \"\"\"\n        Handle command timeout\n        \"\"\"\n        try:\n            self.recording = False\n            self.get_logger().info('Command recording timeout')\n\n            # Publish timeout status\n            status_msg = String()\n            status_msg.data = 'command_timeout'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in timeout callback: {e}')\n\n    def process_audio(self):\n        \"\"\"\n        Process audio buffer for voice commands\n        \"\"\"\n        if not self.whisper_model or not self.recording:\n            return\n\n        try:\n            # Collect audio from buffer\n            audio_chunks = []\n            buffer_size = self.audio_buffer.qsize()\n\n            # Get all available audio chunks\n            for _ in range(buffer_size):\n                try:\n                    chunk_data = self.audio_buffer.get_nowait()\n                    audio_chunks.append(chunk_data['audio'])\n                except queue.Empty:\n                    break\n\n            if not audio_chunks:\n                return\n\n            # Concatenate audio chunks\n            full_audio = np.concatenate(audio_chunks)\n\n            # Check if we have enough audio (at least 1 second)\n            min_samples = int(self.sample_rate * 0.5)  # 0.5 seconds minimum\n            if len(full_audio) < min_samples:\n                return\n\n            self.get_logger().debug(f'Processing {len(full_audio)/self.sample_rate:.2f}s of audio')\n\n            # Transcribe using Whisper\n            with self.audio_lock:\n                result = self.whisper_model.transcribe(full_audio, fp16=False)\n\n            # Check confidence (Whisper doesn't provide confidence, so we'll use alternative measures)\n            transcription = result['text'].strip()\n\n            if transcription and len(transcription) > 3:  # Basic validity check\n                confidence = self._estimate_confidence(transcription, full_audio)\n\n                if confidence >= self.min_confidence:\n                    # Create voice command\n                    voice_command = VoiceCommand(\n                        text=transcription,\n                        confidence=confidence,\n                        timestamp=time.time()\n                    )\n\n                    # Interpret the command\n                    interpreted_command = self.interpret_command(transcription)\n                    voice_command.interpreted_action = interpreted_command\n\n                    # Add to command queue\n                    if not self.command_queue.full():\n                        self.command_queue.put(voice_command)\n                        self.get_logger().info(f'Recognized command: \"{transcription}\" (confidence: {confidence:.2f})')\n\n                        # Publish feedback\n                        feedback_msg = String()\n                        feedback_msg.data = f'Command recognized: {transcription}'\n                        self.feedback_pub.publish(feedback_msg)\n\n                        # Stop recording after successful recognition\n                        self.recording = False\n                        if self.command_timeout_timer:\n                            self.command_timeout_timer.cancel()\n\n                        # Update statistics\n                        self.processed_commands += 1\n                    else:\n                        self.get_logger().warn('Command queue is full, dropping command')\n                else:\n                    self.get_logger().debug(f'Command below confidence threshold: {transcription}')\n            else:\n                self.get_logger().debug('Empty or invalid transcription')\n\n        except Exception as e:\n            self.recording = False\n            self.get_logger().error(f'Error in audio processing: {e}')\n            self.recognition_errors += 1\n\n    def _estimate_confidence(self, text: str, audio: np.ndarray) -> float:\n        \"\"\"\n        Estimate confidence of transcription (simplified approach)\n        \"\"\"\n        # In a real implementation, you might use more sophisticated methods\n        # For now, we'll use simple heuristics\n\n        # Check for common filler words that might indicate low confidence\n        filler_words = ['um', 'uh', 'ah', 'you know', 'like']\n        filler_count = sum(1 for word in filler_words if word in text.lower())\n\n        # Check audio quality (simplified)\n        audio_energy = np.mean(np.abs(audio))\n        has_speech = audio_energy > 0.01  # Threshold for speech detection\n\n        # Calculate confidence score\n        confidence = 0.9  # Base confidence\n\n        if filler_count > 0:\n            confidence -= 0.1 * filler_count\n\n        if not has_speech:\n            confidence -= 0.2\n\n        # Ensure confidence is between 0 and 1\n        return max(0.0, min(1.0, confidence))\n\n    def interpret_command(self, text: str) -> Optional[str]:\n        \"\"\"\n        Interpret natural language command into robot action\n        \"\"\"\n        text_lower = text.lower()\n\n        # Define command patterns\n        command_patterns = {\n            r'move forward|go forward|move ahead|go ahead': 'move_forward',\n            r'move backward|go backward|move back|go back': 'move_backward',\n            r'turn left|rotate left': 'turn_left',\n            r'turn right|rotate right': 'turn_right',\n            r'stop|halt|cease': 'stop',\n            r'pick up|grasp|take|get|lift': 'grasp_object',\n            r'put down|release|drop': 'release_object',\n            r'approach|go to|move to': 'approach_object',\n            r'help|what can you do': 'show_help',\n            r'hello|hi|hey': 'greet',\n        }\n\n        for pattern, action in command_patterns.items():\n            if re.search(pattern, text_lower):\n                return action\n\n        # If no specific pattern matches, return a generic command\n        return 'unknown_command'\n\n    def monitor_system(self):\n        \"\"\"\n        Monitor system status and performance\n        \"\"\"\n        try:\n            status_msg = String()\n            status_msg.data = f'Whisper Active: {self.whisper_model is not None}, ' \\\n                             f'Recording: {self.recording}, ' \\\n                             f'Commands: {self.processed_commands}, ' \\\n                             f'Errors: {self.recognition_errors}, ' \\\n                             f'Buffer Size: {self.audio_buffer.qsize()}'\n\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().debug(f'System Status: {status_msg.data}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in monitoring: {e}')\n\n    def destroy_node(self):\n        \"\"\"\n        Clean up resources when node is destroyed\n        \"\"\"\n        self.get_logger().info('Cleaning up Whisper Voice Command Node')\n        super().destroy_node()\n\n\nclass VoiceCommandGrammar:\n    \"\"\"\n    Defines grammar and validation for voice commands\n    \"\"\"\n    def __init__(self):\n        self.valid_actions = [\n            'move_forward', 'move_backward', 'turn_left', 'turn_right', 'stop',\n            'grasp_object', 'release_object', 'approach_object', 'navigate_to',\n            'find_object', 'follow_me', 'wait', 'come_here'\n        ]\n\n        self.object_keywords = [\n            'cup', 'box', 'ball', 'table', 'chair', 'door', 'person', 'object'\n        ]\n\n        self.location_keywords = [\n            'kitchen', 'living room', 'bedroom', 'office', 'hallway', 'here', 'there'\n        ]\n\n    def validate_command(self, command_text: str) -> Dict[str, Any]:\n        \"\"\"\n        Validate and parse a voice command\n        \"\"\"\n        result = {\n            'valid': False,\n            'action': None,\n            'object': None,\n            'location': None,\n            'confidence': 0.0,\n            'parsed_text': command_text\n        }\n\n        # Parse the command\n        words = command_text.lower().split()\n\n        # Extract action\n        for word in words:\n            if word in ['move', 'go', 'turn', 'rotate', 'stop', 'pick', 'grasp', 'take', 'put', 'release']:\n                # Map to valid action\n                if word in ['move', 'go']:\n                    if any(w in words for w in ['forward', 'ahead']):\n                        result['action'] = 'move_forward'\n                    elif any(w in words for w in ['backward', 'back']):\n                        result['action'] = 'move_backward'\n                    elif any(w in words for w in ['left', 'right']):\n                        result['action'] = 'turn_left' if 'left' in words else 'turn_right'\n                elif word in ['stop', 'halt']:\n                    result['action'] = 'stop'\n                elif word in ['pick', 'grasp', 'take']:\n                    result['action'] = 'grasp_object'\n                elif word in ['put', 'release', 'drop']:\n                    result['action'] = 'release_object'\n\n        # Extract object\n        for word in words:\n            if word in self.object_keywords:\n                result['object'] = word\n                break\n\n        # Extract location\n        for word in words:\n            if word in self.location_keywords:\n                result['location'] = word\n                break\n\n        # Validate action\n        if result['action'] in self.valid_actions:\n            result['valid'] = True\n            result['confidence'] = 0.8  # Assume 80% confidence for valid commands\n\n        return result\n\n\nclass AdvancedWhisperProcessor:\n    \"\"\"\n    Advanced Whisper processor with additional features\n    \"\"\"\n    def __init__(self, model_size: str = 'base', enable_gpu: bool = True):\n        self.model_size = model_size\n        self.enable_gpu = enable_gpu\n        self.model = None\n        self.grammar = VoiceCommandGrammar()\n\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"\n        Initialize the Whisper model with additional features\n        \"\"\"\n        try:\n            device = 'cuda' if self.enable_gpu and torch.cuda.is_available() else 'cpu'\n            self.model = whisper.load_model(self.model_size).to(device)\n\n            # Additional features\n            self.language_detection_enabled = True\n            self.speech_activity_detection = True\n\n            print(f\"Advanced Whisper processor initialized with {self.model_size} model on {device}\")\n\n        except Exception as e:\n            print(f\"Failed to initialize Whisper model: {e}\")\n            self.model = None\n\n    def transcribe_with_context(self, audio: np.ndarray, context: str = \"\") -> Dict[str, Any]:\n        \"\"\"\n        Transcribe audio with contextual information\n        \"\"\"\n        if not self.model:\n            return {'text': '', 'confidence': 0.0, 'valid': False}\n\n        try:\n            # For now, we'll use the basic transcription\n            # In a real implementation, you might use the context to improve recognition\n            result = self.model.transcribe(audio)\n\n            # Validate with grammar\n            validation = self.grammar.validate_command(result['text'])\n\n            return {\n                'text': result['text'],\n                'confidence': validation['confidence'],\n                'valid': validation['valid'],\n                'action': validation['action'],\n                'object': validation['object'],\n                'location': validation['location']\n            }\n\n        except Exception as e:\n            print(f\"Transcription error: {e}\")\n            return {'text': '', 'confidence': 0.0, 'valid': False}\n\n\ndef create_audio_stream_processor():\n    \"\"\"\n    Create a real-time audio stream processor (for standalone use)\n    \"\"\"\n    class AudioStreamProcessor:\n        def __init__(self, callback: callable):\n            self.callback = callback\n            self.chunk = 1024\n            self.format = pyaudio.paInt16\n            self.channels = 1\n            self.rate = 16000\n            self.p = pyaudio.PyAudio()\n\n            self.stream = self.p.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n\n            self.recording = False\n\n        def start_recording(self):\n            self.recording = True\n            threading.Thread(target=self._record_audio).start()\n\n        def stop_recording(self):\n            self.recording = False\n\n        def _record_audio(self):\n            while self.recording:\n                data = self.stream.read(self.chunk)\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n                self.callback(audio_array)\n\n        def __del__(self):\n            self.stream.stop_stream()\n            self.stream.close()\n            self.p.terminate()\n\n    return AudioStreamProcessor\n\n\ndef main(args=None):\n    \"\"\"\n    Main function for Whisper voice command node\n    \"\"\"\n    rclpy.init(args=args)\n\n    try:\n        whisper_node = WhisperVoiceCommandNode()\n\n        # Example: Simulate some voice commands\n        def simulate_voice_command():\n            commands = [\n                \"Move forward\",\n                \"Turn left\",\n                \"Grasp the cup\",\n                \"Go to the kitchen\"\n            ]\n            # In a real system, this would come from actual voice input\n            pass\n\n        rclpy.spin(whisper_node)\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        if 'whisper_node' in locals():\n            whisper_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-whisper-integration-example",children:"Advanced Whisper Integration Example"}),"\n",(0,s.jsx)(n.p,{children:"Here's an example of advanced Whisper integration with additional features:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import asyncio\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\nimport json\nfrom typing import Callable, Awaitable\n\n\nclass RealTimeWhisperController:\n    \"\"\"\n    Real-time Whisper controller for voice commands\n    \"\"\"\n    def __init__(self, model_size: str = 'base', sample_rate: int = 16000):\n        self.model_size = model_size\n        self.sample_rate = sample_rate\n        self.model = None\n        self.executor = ThreadPoolExecutor(max_workers=2)\n\n        # Audio processing\n        self.audio_buffer = []\n        self.is_listening = False\n        self.listening_thread = None\n\n        # Command processing\n        self.command_callbacks = []\n        self.status_callbacks = []\n\n        # Initialize model\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"\n        Initialize the Whisper model\n        \"\"\"\n        try:\n            import whisper\n            self.model = whisper.load_model(self.model_size)\n            print(f\"Real-time Whisper controller initialized with {self.model_size} model\")\n        except ImportError:\n            print(\"Whisper not available, using mock implementation\")\n            self.model = None\n\n    def start_listening(self):\n        \"\"\"\n        Start listening for voice commands\n        \"\"\"\n        if not self.is_listening:\n            self.is_listening = True\n            self.listening_thread = threading.Thread(target=self._listening_loop)\n            self.listening_thread.start()\n\n    def stop_listening(self):\n        \"\"\"\n        Stop listening for voice commands\n        \"\"\"\n        self.is_listening = False\n        if self.listening_thread:\n            self.listening_thread.join()\n\n    def _listening_loop(self):\n        \"\"\"\n        Main listening loop\n        \"\"\"\n        while self.is_listening:\n            # In a real implementation, this would continuously capture audio\n            # For simulation, we'll just sleep and process any buffered audio\n            time.sleep(0.1)\n\n            # Process buffered audio if available\n            if len(self.audio_buffer) > int(self.sample_rate * 2):  # 2 seconds of audio\n                self._process_audio_buffer()\n\n    def _process_audio_buffer(self):\n        \"\"\"\n        Process the accumulated audio buffer\n        \"\"\"\n        if not self.model or len(self.audio_buffer) == 0:\n            return\n\n        try:\n            # Convert to numpy array\n            audio_data = np.array(self.audio_buffer)\n\n            # Process with Whisper (in a separate thread to avoid blocking)\n            future = self.executor.submit(self._transcribe_audio, audio_data)\n            result = future.result(timeout=5.0)  # 5 second timeout\n\n            if result and result['text'].strip():\n                # Notify command callbacks\n                for callback in self.command_callbacks:\n                    callback(result)\n\n                # Update status\n                status = {\n                    'status': 'command_recognized',\n                    'command': result['text'],\n                    'timestamp': time.time()\n                }\n                for callback in self.status_callbacks:\n                    callback(status)\n\n            # Clear buffer after processing\n            self.audio_buffer = []\n\n        except Exception as e:\n            print(f\"Error processing audio buffer: {e}\")\n            status = {\n                'status': 'processing_error',\n                'error': str(e),\n                'timestamp': time.time()\n            }\n            for callback in self.status_callbacks:\n                callback(status)\n\n    def _transcribe_audio(self, audio_data: np.ndarray) -> Dict[str, Any]:\n        \"\"\"\n        Transcribe audio using Whisper\n        \"\"\"\n        if not self.model:\n            return {'text': '', 'confidence': 0.0}\n\n        try:\n            result = self.model.transcribe(audio_data)\n            return {\n                'text': result['text'],\n                'confidence': 0.9,  # Whisper doesn't provide confidence, assume high\n                'language': result.get('language', 'unknown')\n            }\n        except Exception as e:\n            print(f\"Transcription error: {e}\")\n            return {'text': '', 'confidence': 0.0}\n\n    def add_command_callback(self, callback: Callable[[Dict[str, Any]], None]):\n        \"\"\"\n        Add a callback for recognized commands\n        \"\"\"\n        self.command_callbacks.append(callback)\n\n    def add_status_callback(self, callback: Callable[[Dict[str, Any]], None]):\n        \"\"\"\n        Add a callback for status updates\n        \"\"\"\n        self.status_callbacks.append(callback)\n\n    def process_audio_chunk(self, audio_chunk: np.ndarray):\n        \"\"\"\n        Process an incoming audio chunk\n        \"\"\"\n        # Add to buffer\n        self.audio_buffer.extend(audio_chunk.tolist())\n\n        # Limit buffer size to prevent memory issues\n        max_buffer_size = int(self.sample_rate * 10)  # 10 seconds max\n        if len(self.audio_buffer) > max_buffer_size:\n            self.audio_buffer = self.audio_buffer[-max_buffer_size:]\n\n\nclass VoiceCommandManager:\n    \"\"\"\n    Manages voice commands and their execution\n    \"\"\"\n    def __init__(self):\n        self.active_commands = {}\n        self.command_history = []\n        self.max_history = 50\n\n    def register_command(self, command_id: str, command_data: Dict[str, Any]):\n        \"\"\"\n        Register a new voice command\n        \"\"\"\n        self.active_commands[command_id] = {\n            'data': command_data,\n            'timestamp': time.time(),\n            'status': 'registered'\n        }\n\n        # Add to history\n        self.command_history.append({\n            'id': command_id,\n            'data': command_data,\n            'timestamp': time.time()\n        })\n\n        # Limit history size\n        if len(self.command_history) > self.max_history:\n            self.command_history = self.command_history[-self.max_history:]\n\n    def execute_command(self, command_id: str) -> bool:\n        \"\"\"\n        Execute a registered command\n        \"\"\"\n        if command_id not in self.active_commands:\n            return False\n\n        command = self.active_commands[command_id]\n        command['status'] = 'executing'\n\n        # In a real implementation, this would execute the command\n        # For simulation, we'll just update the status\n        time.sleep(0.1)  # Simulate execution time\n\n        command['status'] = 'completed'\n        command['completion_time'] = time.time()\n\n        return True\n\n    def get_command_status(self, command_id: str) -> str:\n        \"\"\"\n        Get the status of a command\n        \"\"\"\n        if command_id in self.active_commands:\n            return self.active_commands[command_id]['status']\n        return 'unknown'\n\n    def get_recent_commands(self, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get recent commands from history\n        \"\"\"\n        return self.command_history[-limit:]\n\n\ndef create_voice_command_config():\n    \"\"\"\n    Create configuration for voice command system\n    \"\"\"\n    config = {\n        # Whisper parameters\n        'model_size': 'base',\n        'enable_gpu': True,\n        'sample_rate': 16000,\n        'audio_chunk_size': 1024,\n\n        # Processing parameters\n        'processing_rate': 2.0,  # Process every 2 seconds\n        'command_timeout': 5.0,\n        'min_confidence': 0.7,\n\n        # Audio parameters\n        'noise_threshold': 0.01,\n        'silence_duration': 1.0,\n        'max_command_duration': 10.0,\n\n        # Recognition parameters\n        'language': 'en',\n        'suppress_tokens': [-1],\n        'temperature': 0.0,\n\n        # Robot integration\n        'command_topic': '/robot/command',\n        'status_topic': '/whisper/status',\n        'feedback_topic': '/whisper/feedback',\n\n        # Debug parameters\n        'enable_logging': True,\n        'log_level': 'INFO',\n        'enable_profiling': False\n    }\n\n    return config\n\n\ndef main_advanced():\n    \"\"\"\n    Main function for advanced Whisper integration\n    \"\"\"\n    print(\"Advanced Whisper Voice Command System\")\n\n    # Create controller\n    controller = RealTimeWhisperController(model_size='base')\n\n    # Create command manager\n    command_manager = VoiceCommandManager()\n\n    # Add command callback\n    def command_callback(result):\n        print(f\"Recognized command: {result['text']}\")\n\n        # Register the command\n        command_id = f\"cmd_{int(time.time())}\"\n        command_manager.register_command(command_id, result)\n\n        # Execute the command\n        success = command_manager.execute_command(command_id)\n        print(f\"Command execution {'successful' if success else 'failed'}\")\n\n    controller.add_command_callback(command_callback)\n\n    # Add status callback\n    def status_callback(status):\n        print(f\"Status update: {status['status']}\")\n\n    controller.add_status_callback(status_callback)\n\n    # Start listening\n    controller.start_listening()\n\n    # Simulate some audio processing\n    try:\n        for i in range(10):\n            # Simulate adding audio chunks\n            dummy_audio = np.random.normal(0, 0.01, 1600)  # 0.1 seconds of audio\n            controller.process_audio_chunk(dummy_audio)\n            time.sleep(0.5)\n\n        # Print recent commands\n        recent_commands = command_manager.get_recent_commands()\n        print(f\"\\nRecent commands: {len(recent_commands)}\")\n        for cmd in recent_commands:\n            print(f\"  - {cmd['data'].get('text', 'unknown')}\")\n\n    except KeyboardInterrupt:\n        print(\"Stopping...\")\n\n    finally:\n        controller.stop_listening()\n\n\nif __name__ == \"__main__\":\n    main_advanced()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-by-step-practical-tutorial",children:"Step-by-Step Practical Tutorial"}),"\n",(0,s.jsx)(n.h3,{id:"implementing-whisper-voice-command-integration",children:"Implementing Whisper Voice Command Integration"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Install Whisper and required dependencies"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip3 install openai-whisper torch torchaudio pyaudio\n# On some systems, you might need additional packages:\nsudo apt update\nsudo apt install portaudio19-dev python3-pyaudio\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create a Whisper integration package"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python whisper_integration_examples --dependencies rclpy std_msgs sensor_msgs geometry_msgs sensor_msgs\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Navigate to the package directory"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd whisper_integration_examples\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create the main module directory"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir whisper_integration_examples\ntouch whisper_integration_examples/__init__.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create the Whisper integration implementation"})," (",(0,s.jsx)(n.code,{children:"whisper_integration_examples/whisper_integration.py"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Use the Whisper integration code examples above\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create a configuration file"})," (",(0,s.jsx)(n.code,{children:"config/whisper_config.yaml"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'whisper_voice_command_node:\n  ros__parameters:\n    # Whisper model parameters\n    model_size: "base"  # tiny, base, small, medium, large\n    enable_gpu_processing: true\n    sample_rate: 16000\n    audio_chunk_size: 1024\n\n    # Processing parameters\n    processing_rate: 2.0\n    command_timeout: 5.0\n    min_confidence: 0.7\n\n    # Audio parameters\n    noise_threshold: 0.01\n    silence_duration: 1.0\n\n    # Topic configuration\n    audio_topic: "/microphone/audio_raw"\n    command_topic: "/robot/command"\n    status_topic: "/whisper/status"\n    feedback_topic: "/whisper/feedback"\n\n    # Debug parameters\n    enable_logging: true\n    log_level: "INFO"\n    enable_profiling: false\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create launch directory"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir launch\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create a launch file"})," (",(0,s.jsx)(n.code,{children:"launch/whisper_integration_example.launch.py"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    enable_gpu = LaunchConfiguration('enable_gpu', default='true')\n\n    # Get package share directory\n    pkg_share = get_package_share_directory('whisper_integration_examples')\n    config_file = os.path.join(pkg_share, 'config', 'whisper_config.yaml')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n        DeclareLaunchArgument(\n            'enable_gpu',\n            default_value='true',\n            description='Enable GPU processing'),\n\n        # Whisper voice command node\n        Node(\n            package='whisper_integration_examples',\n            executable='whisper_integration_examples.whisper_integration',\n            name='whisper_voice_command_node',\n            parameters=[\n                config_file,\n                {'use_sim_time': use_sim_time},\n                {'enable_gpu_processing': enable_gpu}\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Update setup.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'whisper_integration_examples'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\n        (os.path.join('share', package_name, 'config'), glob('config/*.yaml')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='User',\n    maintainer_email='user@example.com',\n    description='Whisper integration examples for voice commands',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'whisper_integration_node = whisper_integration_examples.whisper_integration:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Build the package"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select whisper_integration_examples\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Source the workspace"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Run the Whisper integration example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch whisper_integration_examples whisper_integration_example.launch.py enable_gpu:=true\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Test with simulated audio input"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# In another terminal, you would publish audio data\n# For testing, you can simulate by publishing text commands to check the pipeline\nros2 topic pub /microphone/audio_raw sensor_msgs/msg/AudioData "data: [72, 101, 108, 108, 111]"  # "Hello" in ASCII\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Monitor the system status"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /whisper/status\nros2 topic echo /whisper/feedback\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered the integration of OpenAI Whisper for voice command recognition in robotic systems. We explored the architecture of Whisper models, implementation of real-time voice command processing pipelines, and techniques for optimizing voice recognition for robotics applications."}),"\n",(0,s.jsx)(n.p,{children:"Whisper integration enables robots to understand natural spoken language, providing an intuitive interaction paradigm. The examples demonstrated how to build robust voice command systems that can handle various acoustic conditions and provide reliable command recognition for robotic control."}),"\n",(0,s.jsx)(n.h2,{id:"mini-quiz",children:"Mini-Quiz"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is the primary purpose of Whisper in robotics applications?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Image recognition"}),"\n",(0,s.jsx)(n.li,{children:"B) Speech recognition and transcription"}),"\n",(0,s.jsx)(n.li,{children:"C) Motion planning"}),"\n",(0,s.jsx)(n.li,{children:"D) Path finding"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Which parameter controls the size of the Whisper model?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) model_type"}),"\n",(0,s.jsx)(n.li,{children:"B) model_size"}),"\n",(0,s.jsx)(n.li,{children:"C) model_version"}),"\n",(0,s.jsx)(n.li,{children:"D) model_scale"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is an important consideration for real-time voice command processing?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) High storage requirements"}),"\n",(0,s.jsx)(n.li,{children:"B) Low latency processing"}),"\n",(0,s.jsx)(n.li,{children:"C) Complex visualization"}),"\n",(0,s.jsx)(n.li,{children:"D) Multiple displays"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Which ROS message type is typically used for audio data?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) AudioStream"}),"\n",(0,s.jsx)(n.li,{children:"B) SoundData"}),"\n",(0,s.jsx)(n.li,{children:"C) AudioData"}),"\n",(0,s.jsx)(n.li,{children:"D) VoiceData"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What should be considered when designing voice command grammars?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Only complex sentences"}),"\n",(0,s.jsx)(n.li,{children:"B) Vocabulary limitations and ambiguity resolution"}),"\n",(0,s.jsx)(n.li,{children:"C) Only technical terms"}),"\n",(0,s.jsx)(n.li,{children:"D) Only long commands"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Answers"}),": 1-B, 2-B, 3-B, 4-C, 5-B"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>t});var a=i(6540);const s={},o=a.createContext(s);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);