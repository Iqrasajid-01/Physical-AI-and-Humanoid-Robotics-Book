"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[7225],{735:(n,e,a)=>{a.d(e,{A:()=>t});const t=a.p+"assets/images/ch22-flow-ea63c07786d95694d4012611d6ac1633.svg"},3080:(n,e,a)=>{a.d(e,{A:()=>t});const t=a.p+"assets/images/ch22-ad-f9adad522b045485eed812b8a0a1046d.svg"},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>r});var t=a(6540);const i={},s=t.createContext(i);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),t.createElement(s.Provider,{value:e},n.children)}},9420:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4_vla/advanced-vla-applications","title":"Advanced VLA Applications","description":"Learning Objectives","source":"@site/docs/module4_vla/22-advanced-vla-applications.md","sourceDirName":"module4_vla","slug":"/module4_vla/advanced-vla-applications","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/advanced-vla-applications","draft":false,"unlisted":false,"editUrl":"https://github.com/Iqrasajid-01/docs/module4_vla/22-advanced-vla-applications.md","tags":[],"version":"current","sidebarPosition":22,"frontMatter":{"title":"Advanced VLA Applications","sidebar_label":"22 - Advanced VLA Applications"},"sidebar":"tutorialSidebar","previous":{"title":"21 - Whisper Integration for Voice Commands","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/whisper-integration-for-voice-commands"},"next":{"title":"23 - Robotics Labs","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/labs_and_projects/23-robotics-labs"}}');var i=a(4848),s=a(8453);const o={title:"Advanced VLA Applications",sidebar_label:"22 - Advanced VLA Applications"},r="Advanced VLA Applications",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Multi-Modal Learning",id:"multi-modal-learning",level:3},{value:"Context-Aware VLA",id:"context-aware-vla",level:3},{value:"Collaborative VLA Applications",id:"collaborative-vla-applications",level:3},{value:"Dynamic Environment Adaptation",id:"dynamic-environment-adaptation",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Flow Diagram",id:"flow-diagram",level:2},{value:"Code Example: Advanced VLA Application Framework",id:"code-example-advanced-vla-application-framework",level:2},{value:"Advanced VLA Application: Humanoid Task Execution",id:"advanced-vla-application-humanoid-task-execution",level:2},{value:"Step-by-Step Practical Tutorial",id:"step-by-step-practical-tutorial",level:2},{value:"Implementing Advanced VLA Applications",id:"implementing-advanced-vla-applications",level:3},{value:"Summary",id:"summary",level:2},{value:"Mini-Quiz",id:"mini-quiz",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"advanced-vla-applications",children:"Advanced VLA Applications"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Design complex VLA applications for humanoid robotics"}),"\n",(0,i.jsx)(e.li,{children:"Implement multi-modal learning and adaptation systems"}),"\n",(0,i.jsx)(e.li,{children:"Create collaborative human-robot interaction scenarios"}),"\n",(0,i.jsx)(e.li,{children:"Develop VLA systems for dynamic and unstructured environments"}),"\n",(0,i.jsx)(e.li,{children:"Integrate VLA with advanced perception and manipulation systems"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate and optimize VLA performance in real-world applications"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Advanced VLA (Vision-Language-Action) applications represent the frontier of AI-powered robotics, where sophisticated integration of perception, language understanding, and action execution enables robots to perform complex tasks in real-world environments. These applications go beyond simple command execution to include learning, adaptation, and collaborative interaction with humans."}),"\n",(0,i.jsx)(e.p,{children:"Advanced VLA systems are particularly valuable for humanoid robotics, where robots need to operate in human-centric environments and understand complex, context-dependent instructions. This chapter explores sophisticated applications that push the boundaries of what's possible with VLA technology."}),"\n",(0,i.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsx)(e.h3,{id:"multi-modal-learning",children:"Multi-Modal Learning"}),"\n",(0,i.jsx)(e.p,{children:"Advanced VLA systems incorporate:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Self-Supervised Learning"}),": Learning from unlabeled environment interactions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Imitation Learning"}),": Learning from human demonstrations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning through trial and error with rewards"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Transfer Learning"}),": Applying learned skills to new tasks and environments"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"context-aware-vla",children:"Context-Aware VLA"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Spatial Reasoning"}),": Understanding spatial relationships and configurations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Temporal Reasoning"}),": Understanding sequences and timing of actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Social Reasoning"}),": Understanding human intentions and social norms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contextual Adaptation"}),": Adjusting behavior based on environmental context"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"collaborative-vla-applications",children:"Collaborative VLA Applications"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-Robot Collaboration"}),": Working together on complex tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Shared Autonomy"}),": Combining human guidance with autonomous execution"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Active Learning"}),": Robots requesting clarification when uncertain"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Teaching Interfaces"}),": Humans teaching new tasks to robots"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"dynamic-environment-adaptation",children:"Dynamic Environment Adaptation"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Online Learning"}),": Adapting to new objects and situations in real-time"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain Randomization"}),": Handling variations in environments and objects"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Uncertainty Management"}),": Dealing with ambiguous or incomplete information"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Failure Recovery"}),": Handling and recovering from execution failures"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"Flow Diagram",src:a(3080).A+"",width:"1657",height:"1188"})}),"\n",(0,i.jsx)(e.h2,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"Flow Diagram",src:a(735).A+"",width:"1300",height:"1029"})}),"\n",(0,i.jsx)(e.h2,{id:"code-example-advanced-vla-application-framework",children:"Code Example: Advanced VLA Application Framework"}),"\n",(0,i.jsx)(e.p,{children:"Here's an example implementation of an advanced VLA application framework:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\nfrom std_msgs.msg import String, Bool, Float32\nfrom geometry_msgs.msg import Pose, Point, Vector3\nfrom nav_msgs.msg import OccupancyGrid\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom cv_bridge import CvBridge\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom typing import Dict, List, Tuple, Optional, Any, Callable\nimport threading\nimport queue\nimport time\nimport json\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport random\n\n\nclass TaskType(Enum):\n    \"\"\"Types of tasks for advanced VLA applications\"\"\"\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    INSPECTION = \"inspection\"\n    COLLABORATION = \"collaboration\"\n    LEARNING = \"learning\"\n\n\nclass ExecutionState(Enum):\n    \"\"\"States for task execution\"\"\"\n    IDLE = \"idle\"\n    PLANNING = \"planning\"\n    EXECUTING = \"executing\"\n    WAITING = \"waiting\"\n    RECOVERING = \"recovering\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass VLAState:\n    \"\"\"State representation for advanced VLA system\"\"\"\n    robot_pose: Pose\n    detected_objects: Dict[str, Pose]\n    human_poses: List[Pose]\n    environment_map: Optional[np.ndarray]\n    task_context: Dict[str, Any]\n    execution_state: ExecutionState\n    confidence_scores: Dict[str, float]\n\n\nclass AdvancedVLAModel(nn.Module):\n    \"\"\"\n    Advanced VLA model with multi-modal processing and learning capabilities\n    \"\"\"\n    def __init__(self,\n                 vision_dim: int = 512,\n                 language_dim: int = 512,\n                 action_dim: int = 8,\n                 hidden_dim: int = 256,\n                 num_layers: int = 4):\n        super().__init__()\n\n        # Vision encoder (CNN-based)\n        self.vision_encoder = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, hidden_dim),  # Adjust based on input size\n            nn.ReLU()\n        )\n\n        # Language encoder (Transformer-based)\n        self.language_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=language_dim, nhead=8, dim_feedforward=hidden_dim),\n            num_layers=num_layers\n        )\n        self.lang_proj = nn.Linear(language_dim, hidden_dim)\n\n        # Multi-modal fusion\n        self.fusion = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=hidden_dim * 2, nhead=8, dim_feedforward=hidden_dim),\n            num_layers=2\n        )\n        self.fusion_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n\n        # Action decoder\n        self.action_decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, action_dim)\n        )\n\n        # Value estimation for learning\n        self.value_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n\n        # Task type classifier\n        self.task_classifier = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, len(TaskType))\n        )\n\n    def forward(self,\n                vision_input: torch.Tensor,\n                language_input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the advanced VLA model\n        \"\"\"\n        # Encode vision\n        vision_features = self.vision_encoder(vision_input)\n\n        # Encode language (simplified - in practice, language would be tokenized)\n        lang_features = self.lang_proj(language_input)\n        lang_encoded = self.language_encoder(lang_features.unsqueeze(1)).squeeze(1)\n\n        # Fuse modalities\n        fused_input = torch.cat([vision_features, lang_features], dim=-1)\n        fused_features = self.fusion_proj(fused_input)\n\n        # Decode actions\n        actions = self.action_decoder(fused_features)\n\n        # Estimate value\n        value = self.value_head(fused_features)\n\n        # Classify task type\n        task_probs = F.softmax(self.task_classifier(fused_features), dim=-1)\n\n        return actions, value, task_probs\n\n\nclass AdvancedVLAPlanner:\n    \"\"\"\n    Advanced planner with learning and adaptation capabilities\n    \"\"\"\n    def __init__(self,\n                 model: AdvancedVLAModel,\n                 device: torch.device = torch.device('cpu')):\n        self.model = model\n        self.device = device\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n        # Experience replay buffer\n        self.replay_buffer = []\n        self.max_buffer_size = 10000\n\n        # Learning parameters\n        self.gamma = 0.99  # Discount factor\n        self.epsilon = 0.1  # Exploration rate\n\n    def plan_with_learning(self,\n                          visual_input: np.ndarray,\n                          language_command: str,\n                          current_state: VLAState) -> Tuple[List[int], float]:\n        \"\"\"\n        Plan actions with integrated learning\n        \"\"\"\n        # Convert inputs to tensors\n        vision_tensor = torch.from_numpy(visual_input).float().unsqueeze(0).to(self.device)\n        lang_tensor = self._encode_language(language_command).to(self.device)\n\n        # Get model predictions\n        with torch.no_grad():\n            actions, value, task_probs = self.model(vision_tensor, lang_tensor)\n\n        # Convert to action sequence\n        action_probs = F.softmax(actions, dim=-1)\n        action_dist = torch.distributions.Categorical(action_probs)\n        action_sequence = [action_dist.sample().item()]\n\n        return action_sequence, value.item()\n\n    def _encode_language(self, text: str) -> torch.Tensor:\n        \"\"\"\n        Encode language text (simplified)\n        \"\"\"\n        # In a real implementation, this would use proper tokenization\n        # For this example, we'll create a dummy encoding\n        embedding = torch.randn(512)  # 512-dim embedding\n        return embedding\n\n    def update_with_experience(self,\n                              vision_input: torch.Tensor,\n                              language_input: torch.Tensor,\n                              action_taken: int,\n                              reward: float,\n                              next_vision: torch.Tensor,\n                              done: bool):\n        \"\"\"\n        Update model with experience (reinforcement learning)\n        \"\"\"\n        # Add to replay buffer\n        experience = {\n            'vision': vision_input,\n            'language': language_input,\n            'action': action_taken,\n            'reward': reward,\n            'next_vision': next_vision,\n            'done': done\n        }\n\n        self.replay_buffer.append(experience)\n\n        # Maintain buffer size\n        if len(self.replay_buffer) > self.max_buffer_size:\n            self.replay_buffer.pop(0)\n\n        # Train on a batch from replay buffer\n        if len(self.replay_buffer) > 32:  # Minimum batch size\n            self._train_on_batch()\n\n    def _train_on_batch(self):\n        \"\"\"\n        Train on a batch of experiences\n        \"\"\"\n        # Sample random batch\n        batch_size = min(32, len(self.replay_buffer))\n        batch_indices = random.sample(range(len(self.replay_buffer)), batch_size)\n        batch = [self.replay_buffer[i] for i in batch_indices]\n\n        # Prepare batch tensors\n        vision_batch = torch.stack([exp['vision'] for exp in batch]).to(self.device)\n        language_batch = torch.stack([exp['language'] for exp in batch]).to(self.device)\n        actions_batch = torch.tensor([exp['action'] for exp in batch]).to(self.device)\n        rewards_batch = torch.tensor([exp['reward'] for exp in batch]).float().to(self.device)\n\n        # Forward pass\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        actions_pred, values, _ = self.model(vision_batch, language_batch)\n\n        # Compute loss (simplified)\n        action_loss = F.cross_entropy(actions_pred, actions_batch)\n        value_loss = F.mse_loss(values.squeeze(), rewards_batch)\n        total_loss = action_loss + 0.5 * value_loss\n\n        total_loss.backward()\n        self.optimizer.step()\n\n\nclass AdvancedVLANode(Node):\n    \"\"\"\n    Advanced VLA node with learning and adaptation capabilities\n    \"\"\"\n    def __init__(self):\n        super().__init__('advanced_vla_node')\n\n        # Initialize parameters\n        self.declare_parameter('enable_learning', True)\n        self.declare_parameter('learning_rate', 1e-4)\n        self.declare_parameter('replay_buffer_size', 10000)\n        self.declare_parameter('exploration_rate', 0.1)\n        self.declare_parameter('enable_adaptation', True)\n\n        # Get parameters\n        self.enable_learning = self.get_parameter('enable_learning').value\n        self.learning_rate = self.get_parameter('learning_rate').value\n        self.replay_buffer_size = self.get_parameter('replay_buffer_size').value\n        self.exploration_rate = self.get_parameter('exploration_rate').value\n        self.enable_adaptation = self.get_parameter('enable_adaptation').value\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Initialize TF listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Initialize VLA components\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.vla_model = AdvancedVLAModel().to(self.device)\n        self.vla_planner = AdvancedVLAPlanner(self.vla_model, self.device)\n\n        # System state\n        self.current_state = VLAState(\n            robot_pose=Pose(),\n            detected_objects={},\n            human_poses=[],\n            environment_map=None,\n            task_context={},\n            execution_state=ExecutionState.IDLE,\n            confidence_scores={}\n        )\n\n        self.active_task: Optional[str] = None\n        self.task_history: List[Dict] = []\n\n        # Queues for processing\n        self.vision_queue = queue.Queue(maxsize=10)\n        self.command_queue = queue.Queue(maxsize=5)\n        self.feedback_queue = queue.Queue(maxsize=10)\n\n        # Create publishers\n        self.action_pub = self.create_publisher(String, '/vla/action', 10)\n        self.status_pub = self.create_publisher(String, '/advanced_vla/status', 10)\n        self.feedback_pub = self.create_publisher(String, '/advanced_vla/feedback', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/advanced_vla/markers', 10)\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, '/vla/command', self.command_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n\n        # Create timers\n        self.main_timer = self.create_timer(0.1, self.main_control_loop)\n        self.learning_timer = self.create_timer(1.0, self.learning_update)\n        self.monitoring_timer = self.create_timer(2.0, self.monitor_system)\n\n        # Statistics\n        self.total_tasks_completed = 0\n        self.total_learning_updates = 0\n        self.start_time = time.time()\n\n        self.get_logger().info(\n            f'Advanced VLA Node initialized with learning: {self.enable_learning}'\n        )\n\n    def image_callback(self, msg: Image):\n        \"\"\"\n        Handle incoming visual data\n        \"\"\"\n        try:\n            # Convert to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Add to processing queue\n            if not self.vision_queue.full():\n                self.vision_queue.put({\n                    'image': cv_image,\n                    'timestamp': time.time(),\n                    'header': msg.header\n                })\n            else:\n                # Drop oldest if full\n                try:\n                    self.vision_queue.get_nowait()\n                    self.vision_queue.put({\n                        'image': cv_image,\n                        'timestamp': time.time(),\n                        'header': msg.header\n                    })\n                except queue.Empty:\n                    pass\n\n        except Exception as e:\n            self.get_logger().error(f'Error in image callback: {e}')\n\n    def command_callback(self, msg: String):\n        \"\"\"\n        Handle incoming commands\n        \"\"\"\n        try:\n            command = msg.data\n            self.get_logger().info(f'Received command: {command}')\n\n            if not self.command_queue.full():\n                self.command_queue.put({\n                    'command': command,\n                    'timestamp': time.time()\n                })\n            else:\n                self.get_logger().warn('Command queue full, dropping command')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in command callback: {e}')\n\n    def scan_callback(self, msg: LaserScan):\n        \"\"\"\n        Handle laser scan data for environment mapping\n        \"\"\"\n        try:\n            # Process laser scan for obstacle detection\n            ranges = np.array(msg.ranges)\n            angles = np.linspace(msg.angle_min, msg.angle_max, len(ranges))\n\n            # Create simple occupancy grid\n            grid_resolution = 0.1  # 10cm resolution\n            grid_size = 200  # 20m x 20m grid\n            occupancy_grid = np.zeros((grid_size, grid_size))\n\n            # Fill grid with obstacles\n            robot_x, robot_y = grid_size//2, grid_size//2  # Robot at center\n\n            for i, (angle, dist) in enumerate(zip(angles, ranges)):\n                if not (np.isnan(dist) or np.isinf(dist)) and dist < 10.0:  # Within 10m\n                    x = robot_x + int(dist * np.cos(angle) / grid_resolution)\n                    y = robot_y + int(dist * np.sin(angle) / grid_resolution)\n\n                    if 0 <= x < grid_size and 0 <= y < grid_size:\n                        occupancy_grid[x, y] = 1.0  # Obstacle detected\n\n            self.current_state.environment_map = occupancy_grid\n\n        except Exception as e:\n            self.get_logger().error(f'Error in scan callback: {e}')\n\n    def main_control_loop(self):\n        \"\"\"\n        Main control loop for advanced VLA system\n        \"\"\"\n        try:\n            # Process new commands\n            if not self.command_queue.empty():\n                cmd_data = self.command_queue.get()\n                self._process_command(cmd_data['command'])\n\n            # Process visual input\n            if not self.vision_queue.empty():\n                vision_data = self.vision_queue.get()\n                self._process_vision(vision_data['image'])\n\n            # Execute current task if active\n            if self.current_state.execution_state == ExecutionState.PLANNING:\n                self._execute_planning()\n            elif self.current_state.execution_state == ExecutionState.EXECUTING:\n                self._execute_current_task()\n\n        except Exception as e:\n            self.get_logger().error(f'Error in main control loop: {e}')\n\n    def _process_command(self, command: str):\n        \"\"\"\n        Process a natural language command\n        \"\"\"\n        try:\n            self.get_logger().info(f'Processing command: {command}')\n\n            # Update task context\n            self.current_state.task_context = {\n                'command': command,\n                'timestamp': time.time(),\n                'status': 'processing'\n            }\n\n            # Determine task type\n            task_type = self._classify_task(command)\n            self.get_logger().info(f'Classified task as: {task_type}')\n\n            # Set execution state to planning\n            self.current_state.execution_state = ExecutionState.PLANNING\n            self.active_task = command\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f'Planning for task: {command}'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n            self.current_state.execution_state = ExecutionState.FAILED\n\n    def _classify_task(self, command: str) -> TaskType:\n        \"\"\"\n        Classify the type of task from the command\n        \"\"\"\n        command_lower = command.lower()\n\n        if any(word in command_lower for word in ['move', 'go', 'navigate', 'go to']):\n            return TaskType.NAVIGATION\n        elif any(word in command_lower for word in ['pick', 'grasp', 'take', 'place', 'put']):\n            return TaskType.MANIPULATION\n        elif any(word in command_lower for word in ['inspect', 'look', 'check', 'find']):\n            return TaskType.INSPECTION\n        elif any(word in command_lower for word in ['help', 'assist', 'together', 'with me']):\n            return TaskType.COLLABORATION\n        else:\n            return TaskType.NAVIGATION  # Default\n\n    def _process_vision(self, image: np.ndarray):\n        \"\"\"\n        Process visual input and update state\n        \"\"\"\n        try:\n            # In a real implementation, this would run object detection,\n            # pose estimation, and scene understanding\n            # For this example, we'll simulate object detection\n\n            # Simulate detecting objects in the environment\n            # In practice, this would use deep learning models\n            detected_objects = {\n                'table': Pose(),  # Position would be estimated\n                'chair': Pose(),\n                'cup': Pose()\n            }\n\n            self.current_state.detected_objects = detected_objects\n\n            # Update confidence scores\n            self.current_state.confidence_scores['vision'] = 0.85\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing vision: {e}')\n\n    def _execute_planning(self):\n        \"\"\"\n        Execute planning phase for current task\n        \"\"\"\n        try:\n            if not self.active_task:\n                return\n\n            # Get latest visual input\n            if not self.vision_queue.empty():\n                latest_vision = self.vision_queue.queue[-1]['image']\n            else:\n                # Use dummy image if no recent vision\n                latest_vision = np.zeros((480, 640, 3), dtype=np.uint8)\n\n            # Plan with VLA model\n            actions, value = self.vla_planner.plan_with_learning(\n                latest_vision,\n                self.active_task,\n                self.current_state\n            )\n\n            if actions:\n                self.get_logger().info(f'Generated plan with {len(actions)} actions')\n\n                # Execute first action\n                self._execute_action(actions[0])\n\n                # Update state\n                self.current_state.execution_state = ExecutionState.EXECUTING\n            else:\n                self.get_logger().warn('No plan generated')\n                self.current_state.execution_state = ExecutionState.FAILED\n\n        except Exception as e:\n            self.get_logger().error(f'Error in planning execution: {e}')\n            self.current_state.execution_state = ExecutionState.FAILED\n\n    def _execute_current_task(self):\n        \"\"\"\n        Execute the current task\n        \"\"\"\n        try:\n            # In a real implementation, this would execute the planned actions\n            # For this example, we'll simulate execution\n\n            # Simulate task execution\n            if self.active_task:\n                # Publish action command\n                action_msg = String()\n                action_msg.data = f'executing:{self.active_task}'\n                self.action_pub.publish(action_msg)\n\n                # Simulate completion after some time\n                # In reality, this would wait for actual execution feedback\n                self.current_state.execution_state = ExecutionState.COMPLETED\n                self.total_tasks_completed += 1\n\n                # Add to history\n                self.task_history.append({\n                    'task': self.active_task,\n                    'timestamp': time.time(),\n                    'status': 'completed'\n                })\n\n                self.get_logger().info(f'Task completed: {self.active_task}')\n\n                # Publish feedback\n                feedback_msg = String()\n                feedback_msg.data = f'Task completed: {self.active_task}'\n                self.feedback_pub.publish(feedback_msg)\n\n                # Reset for next task\n                self.active_task = None\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing task: {e}')\n            self.current_state.execution_state = ExecutionState.FAILED\n\n    def _execute_action(self, action_idx: int):\n        \"\"\"\n        Execute a specific action\n        \"\"\"\n        # In a real implementation, this would map action indices to robot commands\n        action_map = {\n            0: 'move_forward',\n            1: 'move_backward',\n            2: 'turn_left',\n            3: 'turn_right',\n            4: 'grasp',\n            5: 'release',\n            6: 'approach',\n            7: 'retreat'\n        }\n\n        action_name = action_map.get(action_idx, 'unknown')\n        self.get_logger().info(f'Executing action: {action_name}')\n\n    def learning_update(self):\n        \"\"\"\n        Periodic learning updates\n        \"\"\"\n        try:\n            if self.enable_learning:\n                # In a real implementation, this would perform learning updates\n                # based on task outcomes and experiences\n                self.total_learning_updates += 1\n\n                self.get_logger().debug(f'Learning update #{self.total_learning_updates}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in learning update: {e}')\n\n    def monitor_system(self):\n        \"\"\"\n        Monitor system status and performance\n        \"\"\"\n        try:\n            # Calculate performance metrics\n            runtime = time.time() - self.start_time\n            tasks_per_hour = (self.total_tasks_completed / runtime) * 3600 if runtime > 0 else 0\n\n            status_msg = String()\n            status_msg.data = (\n                f'State: {self.current_state.execution_state.value}, '\n                f'Tasks: {self.total_tasks_completed}, '\n                f'Learning Updates: {self.total_learning_updates}, '\n                f'Tasks/Hour: {tasks_per_hour:.2f}, '\n                f'Active Task: {self.active_task or \"None\"}'\n            )\n\n            self.status_pub.publish(status_msg)\n            self.get_logger().info(f'System Status: {status_msg.data}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in monitoring: {e}')\n\n    def destroy_node(self):\n        \"\"\"\n        Clean up resources when node is destroyed\n        \"\"\"\n        self.get_logger().info('Cleaning up Advanced VLA Node')\n        super().destroy_node()\n\n\nclass HumanRobotCollaborationManager:\n    \"\"\"\n    Manager for human-robot collaboration scenarios\n    \"\"\"\n    def __init__(self, vla_node: AdvancedVLANode):\n        self.vla_node = vla_node\n        self.human_intention_detector = None\n        self.shared_autonomy_enabled = True\n        self.collaboration_mode = False\n\n    def enable_collaboration_mode(self):\n        \"\"\"\n        Enable human-robot collaboration mode\n        \"\"\"\n        self.collaboration_mode = True\n        self.vla_node.get_logger().info('Collaboration mode enabled')\n\n    def disable_collaboration_mode(self):\n        \"\"\"\n        Disable human-robot collaboration mode\n        \"\"\"\n        self.collaboration_mode = False\n        self.vla_node.get_logger().info('Collaboration mode disabled')\n\n    def handle_human_feedback(self, feedback_type: str, feedback_data: Any):\n        \"\"\"\n        Handle feedback from human collaborator\n        \"\"\"\n        if not self.collaboration_mode:\n            return\n\n        if feedback_type == 'correction':\n            # Adjust plan based on human correction\n            self._apply_correction(feedback_data)\n        elif feedback_type == 'approval':\n            # Continue with current plan\n            self._continue_with_plan()\n        elif feedback_type == 'request_help':\n            # Enter teaching mode\n            self._enter_teaching_mode()\n\n    def _apply_correction(self, correction_data: Any):\n        \"\"\"\n        Apply correction from human collaborator\n        \"\"\"\n        self.vla_node.get_logger().info('Applying human correction')\n        # In implementation, this would modify the current plan\n\n    def _continue_with_plan(self):\n        \"\"\"\n        Continue execution based on human approval\n        \"\"\"\n        self.vla_node.get_logger().info('Continuing with plan based on human approval')\n\n    def _enter_teaching_mode(self):\n        \"\"\"\n        Enter mode where robot learns from human demonstration\n        \"\"\"\n        self.vla_node.get_logger().info('Entering teaching mode')\n\n\ndef create_advanced_vla_config():\n    \"\"\"\n    Create configuration for advanced VLA applications\n    \"\"\"\n    config = {\n        # Model parameters\n        'model_size': 'large',\n        'enable_gpu': True,\n        'vision_dim': 512,\n        'language_dim': 512,\n        'action_dim': 8,\n\n        # Learning parameters\n        'enable_learning': True,\n        'learning_rate': 1e-4,\n        'replay_buffer_size': 10000,\n        'exploration_rate': 0.1,\n        'discount_factor': 0.99,\n\n        # Execution parameters\n        'execution_rate': 10.0,\n        'planning_timeout': 5.0,\n        'max_action_sequence': 50,\n\n        # Collaboration parameters\n        'enable_collaboration': True,\n        'shared_autonomy': True,\n        'human_feedback_timeout': 10.0,\n\n        # Adaptation parameters\n        'enable_adaptation': True,\n        'online_learning_rate': 1e-5,\n        'adaptation_threshold': 0.7,\n\n        # Safety parameters\n        'enable_safety_monitoring': True,\n        'max_velocity': 0.5,\n        'collision_threshold': 0.3,\n\n        # Debug parameters\n        'enable_logging': True,\n        'log_level': 'INFO',\n        'enable_profiling': True\n    }\n\n    return config\n\n\ndef main(args=None):\n    \"\"\"\n    Main function for advanced VLA node\n    \"\"\"\n    rclpy.init(args=args)\n\n    try:\n        advanced_vla_node = AdvancedVLANode()\n\n        # Initialize collaboration manager\n        collab_manager = HumanRobotCollaborationManager(advanced_vla_node)\n\n        # Example: Enable collaboration mode\n        collab_manager.enable_collaboration_mode()\n\n        rclpy.spin(advanced_vla_node)\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        if 'advanced_vla_node' in locals():\n            advanced_vla_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-vla-application-humanoid-task-execution",children:"Advanced VLA Application: Humanoid Task Execution"}),"\n",(0,i.jsx)(e.p,{children:"Here's an example of a specific advanced application:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\nfrom typing import Dict, List, Tuple, Optional\n\n\nclass HumanoidVLAApplication:\n    \"\"\"\n    Advanced VLA application for humanoid robotics tasks\n    \"\"\"\n    def __init__(self, vla_model: AdvancedVLAModel):\n        self.vla_model = vla_model\n        self.executor = ThreadPoolExecutor(max_workers=4)\n\n        # Humanoid-specific parameters\n        self.arm_joints = ['shoulder_pan', 'shoulder_lift', 'elbow_flex', 'wrist_flex']\n        self.base_platform = ['x', 'y', 'theta']\n        self.gripper = 'gripper_joint'\n\n        # Task-specific parameters\n        self.current_task = None\n        self.task_sequence = []\n        self.task_index = 0\n\n        # Learning components\n        self.demonstration_buffer = []\n        self.skill_library = {}\n\n    async def execute_household_task(self, command: str) -> bool:\n        \"\"\"\n        Execute a household task using VLA\n        \"\"\"\n        try:\n            # Parse the command and create task sequence\n            task_sequence = self._parse_household_command(command)\n\n            if not task_sequence:\n                return False\n\n            self.task_sequence = task_sequence\n            self.task_index = 0\n\n            # Execute each task in sequence\n            for i, task in enumerate(task_sequence):\n                self.get_logger().info(f'Executing task {i+1}/{len(task_sequence)}: {task[\"description\"]}')\n\n                success = await self._execute_single_task(task)\n\n                if not success:\n                    self.get_logger().error(f'Task {i+1} failed: {task[\"description\"]}')\n                    return False\n\n            self.get_logger().info('All tasks completed successfully')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing household task: {e}')\n            return False\n\n    def _parse_household_command(self, command: str) -> List[Dict]:\n        \"\"\"\n        Parse household command into executable tasks\n        \"\"\"\n        command_lower = command.lower()\n\n        # Define common household tasks\n        if 'pick up' in command_lower or 'grasp' in command_lower:\n            object_name = self._extract_object_name(command_lower)\n            return [\n                {'type': 'navigation', 'action': 'approach_object', 'object': object_name, 'description': f'Approach {object_name}'},\n                {'type': 'manipulation', 'action': 'grasp', 'object': object_name, 'description': f'Grasp {object_name}'},\n                {'type': 'navigation', 'action': 'move_to', 'location': 'destination', 'description': 'Move to destination'}\n            ]\n\n        elif 'clean' in command_lower or 'wipe' in command_lower:\n            surface = self._extract_surface_name(command_lower)\n            return [\n                {'type': 'navigation', 'action': 'approach_surface', 'surface': surface, 'description': f'Approach {surface}'},\n                {'type': 'manipulation', 'action': 'clean_surface', 'surface': surface, 'description': f'Clean {surface}'}\n            ]\n\n        elif 'set table' in command_lower:\n            return [\n                {'type': 'navigation', 'action': 'approach_kitchen', 'description': 'Go to kitchen'},\n                {'type': 'manipulation', 'action': 'pick_plate', 'description': 'Pick up plate'},\n                {'type': 'navigation', 'action': 'move_to_table', 'description': 'Move to table'},\n                {'type': 'manipulation', 'action': 'place_plate', 'description': 'Place plate on table'}\n            ]\n\n        # Default: unknown command\n        return []\n\n    def _extract_object_name(self, command: str) -> str:\n        \"\"\"\n        Extract object name from command (simplified)\n        \"\"\"\n        # In a real implementation, this would use NLP techniques\n        known_objects = ['cup', 'plate', 'bottle', 'book', 'phone', 'keys', 'glass', 'fork', 'spoon']\n\n        for obj in known_objects:\n            if obj in command:\n                return obj\n\n        return 'object'  # default\n\n    def _extract_surface_name(self, command: str) -> str:\n        \"\"\"\n        Extract surface name from command (simplified)\n        \"\"\"\n        known_surfaces = ['table', 'counter', 'desk', 'stove', 'sink']\n\n        for surface in known_surfaces:\n            if surface in command:\n                return surface\n\n        return 'surface'  # default\n\n    async def _execute_single_task(self, task: Dict) -> bool:\n        \"\"\"\n        Execute a single task asynchronously\n        \"\"\"\n        try:\n            if task['type'] == 'navigation':\n                return await self._execute_navigation_task(task)\n            elif task['type'] == 'manipulation':\n                return await self._execute_manipulation_task(task)\n            else:\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing task: {e}')\n            return False\n\n    async def _execute_navigation_task(self, task: Dict) -> bool:\n        \"\"\"\n        Execute navigation task\n        \"\"\"\n        # In a real implementation, this would interface with navigation stack\n        # For simulation, we'll just sleep\n        await asyncio.sleep(2.0)\n        return True\n\n    async def _execute_manipulation_task(self, task: Dict) -> bool:\n        \"\"\"\n        Execute manipulation task\n        \"\"\"\n        # In a real implementation, this would control manipulator\n        # For simulation, we'll just sleep\n        await asyncio.sleep(3.0)\n        return True\n\n    def learn_from_demonstration(self, demonstration: List[Dict]) -> bool:\n        \"\"\"\n        Learn a new skill from human demonstration\n        \"\"\"\n        try:\n            # Process the demonstration\n            skill_id = f\"skill_{len(self.skill_library) + 1}\"\n\n            # Extract key features from demonstration\n            skill_descriptor = {\n                'actions': [step['action'] for step in demonstration],\n                'objects': list(set([step.get('object', '') for step in demonstration if step.get('object')])),\n                'contexts': list(set([step.get('context', '') for step in demonstration if step.get('context')]))\n            }\n\n            # Store in skill library\n            self.skill_library[skill_id] = {\n                'descriptor': skill_descriptor,\n                'demonstration': demonstration,\n                'success_rate': 0.0,\n                'attempts': 0,\n                'successes': 0\n            }\n\n            print(f\"Learned new skill: {skill_id}\")\n            return True\n\n        except Exception as e:\n            print(f\"Error learning from demonstration: {e}\")\n            return False\n\n    def adapt_to_new_environment(self, environment_features: Dict) -> bool:\n        \"\"\"\n        Adapt VLA model to new environment\n        \"\"\"\n        try:\n            # Update environment-specific parameters\n            # In a real implementation, this would fine-tune the model\n            print(f\"Adapting to new environment with features: {list(environment_features.keys())}\")\n\n            # Update internal models\n            # This would involve online learning techniques\n            return True\n\n        except Exception as e:\n            print(f\"Error adapting to new environment: {e}\")\n            return False\n\n\ndef main_humanoid_app():\n    \"\"\"\n    Main function for humanoid VLA application\n    \"\"\"\n    print(\"Advanced Humanoid VLA Application\")\n\n    # Initialize model\n    vla_model = AdvancedVLAModel()\n\n    # Create application\n    humanoid_app = HumanoidVLAApplication(vla_model)\n\n    # Example tasks\n    household_commands = [\n        \"Pick up the red cup from the table\",\n        \"Clean the kitchen counter\",\n        \"Set the table for dinner\"\n    ]\n\n    # Execute tasks\n    for command in household_commands:\n        print(f\"\\nExecuting command: {command}\")\n        success = asyncio.run(humanoid_app.execute_household_task(command))\n        print(f\"Task {'completed' if success else 'failed'}\")\n\n    # Example: Learn from demonstration\n    demonstration = [\n        {'action': 'approach_object', 'object': 'cup', 'context': 'table'},\n        {'action': 'grasp_object', 'object': 'cup', 'context': 'table'},\n        {'action': 'move_to', 'location': 'kitchen', 'context': 'holding_cup'},\n        {'action': 'place_object', 'object': 'cup', 'context': 'kitchen_sink'}\n    ]\n\n    print(\"\\nLearning from demonstration...\")\n    humanoid_app.learn_from_demonstration(demonstration)\n\n    # Example: Adapt to new environment\n    env_features = {\n        'layout': 'open_concept',\n        'furniture': ['sofa', 'coffee_table', 'kitchen_island'],\n        'lighting': 'variable',\n        'obstacles': ['plants', 'pet_bed']\n    }\n\n    print(f\"\\nAdapting to new environment...\")\n    humanoid_app.adapt_to_new_environment(env_features)\n\n\nif __name__ == \"__main__\":\n    main_humanoid_app()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"step-by-step-practical-tutorial",children:"Step-by-Step Practical Tutorial"}),"\n",(0,i.jsx)(e.h3,{id:"implementing-advanced-vla-applications",children:"Implementing Advanced VLA Applications"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Install advanced dependencies"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"pip3 install torch torchvision torchaudio transformers\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create an advanced VLA package"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python advanced_vla_examples --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs visualization_msgs cv_bridge tf2_ros\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Navigate to the package directory"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"cd advanced_vla_examples\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create the main module directory"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"mkdir advanced_vla_examples\ntouch advanced_vla_examples/__init__.py\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create the advanced VLA implementation"})," (",(0,i.jsx)(e.code,{children:"advanced_vla_examples/advanced_vla.py"}),"):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Use the advanced VLA application code examples above\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create a configuration file"})," (",(0,i.jsx)(e.code,{children:"config/advanced_vla_config.yaml"}),"):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'advanced_vla_node:\n  ros__parameters:\n    # Model parameters\n    enable_learning: true\n    learning_rate: 0.0001\n    replay_buffer_size: 10000\n    exploration_rate: 0.1\n\n    # Execution parameters\n    enable_adaptation: true\n    execution_rate: 10.0\n    planning_timeout: 5.0\n\n    # Collaboration parameters\n    enable_collaboration: true\n    shared_autonomy: true\n\n    # Safety parameters\n    enable_safety_monitoring: true\n    max_velocity: 0.5\n    collision_threshold: 0.3\n\n    # Debug parameters\n    enable_logging: true\n    log_level: "INFO"\n    enable_profiling: true\n'})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create launch directory"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"mkdir launch\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Create a launch file"})," (",(0,i.jsx)(e.code,{children:"launch/advanced_vla_example.launch.py"}),"):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    enable_gpu = LaunchConfiguration('enable_gpu', default='true')\n\n    # Get package share directory\n    pkg_share = get_package_share_directory('advanced_vla_examples')\n    config_file = os.path.join(pkg_share, 'config', 'advanced_vla_config.yaml')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n        DeclareLaunchArgument(\n            'enable_gpu',\n            default_value='true',\n            description='Enable GPU processing'),\n\n        # Advanced VLA node\n        Node(\n            package='advanced_vla_examples',\n            executable='advanced_vla_examples.advanced_vla',\n            name='advanced_vla_node',\n            parameters=[\n                config_file,\n                {'use_sim_time': use_sim_time},\n                {'enable_learning': True}\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Update setup.py"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'advanced_vla_examples'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\n        (os.path.join('share', package_name, 'config'), glob('config/*.yaml')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='User',\n    maintainer_email='user@example.com',\n    description='Advanced VLA examples for robotics',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'advanced_vla_node = advanced_vla_examples.advanced_vla:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Build the package"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select advanced_vla_examples\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Source the workspace"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Run the advanced VLA example"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"ros2 launch advanced_vla_examples advanced_vla_example.launch.py enable_gpu:=true\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Test with sample commands"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# In another terminal\nros2 topic pub /vla/command std_msgs/String \"data: 'Pick up the red cup from the table'\"\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Monitor the advanced VLA status"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"ros2 topic echo /advanced_vla/status\nros2 topic echo /advanced_vla/feedback\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter covered advanced VLA (Vision-Language-Action) applications that push the boundaries of AI-powered robotics. We explored multi-modal learning systems, context-aware applications, collaborative human-robot interaction, and adaptation to dynamic environments."}),"\n",(0,i.jsx)(e.p,{children:"Advanced VLA applications enable robots to perform complex tasks in real-world environments by integrating perception, language understanding, and action execution with learning and adaptation capabilities. These systems represent the future of intelligent robotics, where robots can learn from experience and adapt to new situations."}),"\n",(0,i.jsx)(e.h2,{id:"mini-quiz",children:"Mini-Quiz"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is a key feature of advanced VLA applications?"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A) Simple command execution only"}),"\n",(0,i.jsx)(e.li,{children:"B) Multi-modal learning and adaptation"}),"\n",(0,i.jsx)(e.li,{children:"C) Only visual processing"}),"\n",(0,i.jsx)(e.li,{children:"D) Only pre-programmed tasks"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Which type of learning is important for advanced VLA systems?"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A) Supervised learning only"}),"\n",(0,i.jsx)(e.li,{children:"B) Unsupervised learning only"}),"\n",(0,i.jsx)(e.li,{children:"C) Reinforcement learning and imitation learning"}),"\n",(0,i.jsx)(e.li,{children:"D) Rule-based programming only"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'What does "shared autonomy" refer to in VLA systems?'}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A) Multiple robots sharing tasks"}),"\n",(0,i.jsx)(e.li,{children:"B) Combining human guidance with autonomous execution"}),"\n",(0,i.jsx)(e.li,{children:"C) Sharing computational resources"}),"\n",(0,i.jsx)(e.li,{children:"D) Multiple users controlling one robot"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Which component is important for handling uncertainty in VLA systems?"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A) Only deterministic planning"}),"\n",(0,i.jsx)(e.li,{children:"B) Uncertainty management and failure recovery"}),"\n",(0,i.jsx)(e.li,{children:"C) Only high-speed processing"}),"\n",(0,i.jsx)(e.li,{children:"D) Only visual sensors"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is the purpose of a replay buffer in advanced VLA learning?"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A) To store video recordings"}),"\n",(0,i.jsx)(e.li,{children:"B) To store experiences for reinforcement learning"}),"\n",(0,i.jsx)(e.li,{children:"C) To buffer sensor data only"}),"\n",(0,i.jsx)(e.li,{children:"D) To store configuration files"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answers"}),": 1-B, 2-C, 3-B, 4-B, 5-B"]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);