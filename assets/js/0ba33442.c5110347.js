"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[3658],{1136:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ch13-ad-0737316a70f4838cd4014ad17a009baa.svg"},6727:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ch13-flow-037419f72ef7cbfae52079e023a8e89b.svg"},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},9603:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module3_isaac/isaac-perception-systems","title":"Isaac Perception Systems","description":"Learning Objectives","source":"@site/docs/module3_isaac/13-isaac-perception-systems.md","sourceDirName":"module3_isaac","slug":"/module3_isaac/isaac-perception-systems","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module3_isaac/isaac-perception-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/Iqrasajid-01/docs/module3_isaac/13-isaac-perception-systems.md","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"title":"Isaac Perception Systems","sidebar_label":"13 - Isaac Perception Systems"},"sidebar":"tutorialSidebar","previous":{"title":"12 - Introduction to NVIDIA Isaac","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module3_isaac/12-introduction-to-nvidia-isaac"},"next":{"title":"14 - Isaac Navigation with Nav2","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module3_isaac/isaac-navigation-nav2"}}');var t=i(4848),r=i(8453);const o={title:"Isaac Perception Systems",sidebar_label:"13 - Isaac Perception Systems"},a="Isaac Perception Systems",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Isaac Perception Architecture",id:"isaac-perception-architecture",level:3},{value:"Key Perception Technologies",id:"key-perception-technologies",level:3},{value:"Isaac ROS Perception Packages",id:"isaac-ros-perception-packages",level:3},{value:"Edge Computing Considerations",id:"edge-computing-considerations",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Flow Diagram",id:"flow-diagram",level:2},{value:"Code Example: Isaac Perception Pipeline",id:"code-example-isaac-perception-pipeline",level:2},{value:"Isaac Perception Configuration",id:"isaac-perception-configuration",level:2},{value:"Step-by-Step Practical Tutorial",id:"step-by-step-practical-tutorial",level:2},{value:"Building an Isaac Perception System",id:"building-an-isaac-perception-system",level:3},{value:"Summary",id:"summary",level:2},{value:"Mini-Quiz",id:"mini-quiz",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-perception-systems",children:"Isaac Perception Systems"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the architecture of Isaac perception systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement GPU-accelerated computer vision algorithms using Isaac"}),"\n",(0,t.jsx)(n.li,{children:"Configure and optimize perception pipelines for robotics applications"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multiple sensors for enhanced perception capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Deploy perception models on edge computing platforms like Jetson"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate and validate perception system performance in real-world scenarios"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Perception is a critical component of intelligent robotic systems, enabling robots to understand and interact with their environment. NVIDIA Isaac provides specialized tools and optimized libraries for developing high-performance perception systems that leverage GPU acceleration. These systems can process visual, depth, and other sensory data in real-time to enable robots to navigate, recognize objects, and make intelligent decisions."}),"\n",(0,t.jsx)(n.p,{children:"Isaac's perception capabilities are built on NVIDIA's expertise in computer vision, deep learning, and GPU computing. The platform provides optimized implementations of common perception algorithms and tools for training custom perception models. This chapter explores how to build and deploy perception systems using Isaac's specialized libraries and tools."}),"\n",(0,t.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-perception-architecture",children:"Isaac Perception Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Isaac perception systems are built on several key components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Acceleration"}),": GPU-optimized algorithms for real-time processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning Integration"}),": Integration with NVIDIA's AI frameworks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Processing"}),": Optimized handling of camera, LIDAR, and other sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Pipelines"}),": Modular processing pipelines for different tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-perception-technologies",children:"Key Perception Technologies"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": Identifying and localizing objects in images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Semantic Segmentation"}),": Pixel-level classification of image content"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Instance Segmentation"}),": Object detection with pixel-level masks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pose Estimation"}),": Determining position and orientation of objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Estimation"}),": Extracting 3D information from 2D images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM"}),": Simultaneous localization and mapping"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-perception-packages",children:"Isaac ROS Perception Packages"}),"\n",(0,t.jsx)(n.p,{children:"Isaac provides specialized ROS packages for perception:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_detectnet"}),": Object detection with TensorRT acceleration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_segmentation"}),": Semantic segmentation pipelines"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_pose_estimation"}),": 6D pose estimation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_visual_slam"}),": Visual SLAM implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"isaac_ros_image_pipeline"}),": Image preprocessing and enhancement"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"edge-computing-considerations",children:"Edge Computing Considerations"}),"\n",(0,t.jsx)(n.p,{children:"When deploying perception systems on edge platforms:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Power Efficiency"}),": Optimizing for limited power budgets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),": Ensuring real-time performance requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Optimization"}),": Using TensorRT for inference optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Management"}),": Efficient use of limited memory resources"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Flow Diagram",src:i(1136).A+"",width:"1920",height:"775"})}),"\n",(0,t.jsx)(n.h2,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Flow Diagram",src:i(6727).A+"",width:"1493",height:"381"})}),"\n",(0,t.jsx)(n.h2,{id:"code-example-isaac-perception-pipeline",children:"Code Example: Isaac Perception Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Here's an example of a complete Isaac perception pipeline using GPU acceleration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point, Pose, TransformStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport torch\nimport torchvision.transforms as transforms\nfrom tf2_ros import TransformBroadcaster\nfrom std_msgs.msg import Header\nimport time\n\n\nclass IsaacPerceptionPipeline(Node):\n    \"\"\"\n    Complete Isaac perception pipeline with GPU acceleration\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Initialize transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Load models (example with multiple models)\n        try:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            self.get_logger().info(f'Using device: {self.device}')\n\n            # Load detection model\n            self.detection_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n            self.detection_model.to(self.device)\n            self.detection_model.eval()\n\n            # Load segmentation model\n            self.segmentation_model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n            self.segmentation_model.to(self.device)\n            self.segmentation_model.eval()\n\n            self.get_logger().info('Perception models loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load models: {e}')\n            self.detection_model = None\n            self.segmentation_model = None\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/perception/detections',\n            10\n        )\n\n        self.segmentation_pub = self.create_publisher(\n            Image,  # Publish segmentation mask as image\n            '/perception/segmentation_mask',\n            10\n        )\n\n        # Store camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Processing statistics\n        self.frame_count = 0\n        self.last_process_time = time.time()\n\n        self.get_logger().info('Isaac Perception Pipeline initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"\n        Store camera intrinsic parameters\n        \"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"\n        Process incoming image through perception pipeline\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform detection\n            detections = self.perform_detection(cv_image)\n\n            # Perform segmentation\n            segmentation_mask = self.perform_segmentation(cv_image)\n\n            # Publish results\n            self.publish_detections(detections, msg.header)\n            self.publish_segmentation(segmentation_mask, msg.header)\n\n            # Calculate processing time\n            process_time = time.time() - start_time\n            self.frame_count += 1\n\n            # Log performance metrics every 30 frames\n            if self.frame_count % 30 == 0:\n                avg_time = (time.time() - self.last_process_time) / 30\n                fps = 1.0 / avg_time if avg_time > 0 else 0\n                self.get_logger().info(f'Perception pipeline: {fps:.2f} FPS, {process_time*1000:.2f} ms per frame')\n                self.last_process_time = time.time()\n\n        except Exception as e:\n            self.get_logger().error(f'Error in perception pipeline: {e}')\n\n    def perform_detection(self, image):\n        \"\"\"\n        Perform object detection using YOLOv5\n        \"\"\"\n        if self.detection_model is None:\n            return []\n\n        # Preprocess image\n        img_tensor = self.detection_model.preprocess(image)\n\n        # Perform inference\n        with torch.no_grad():\n            results = self.detection_model(img_tensor)\n\n        # Process results\n        detections = []\n        for *xyxy, conf, cls in results.xyxy[0].tolist():\n            detection = {\n                'bbox': [int(coord) for coord in xyxy],\n                'confidence': conf,\n                'class_id': int(cls),\n                'class_name': self.detection_model.names[int(cls)]\n            }\n            detections.append(detection)\n\n        return detections\n\n    def perform_segmentation(self, image):\n        \"\"\"\n        Perform semantic segmentation\n        \"\"\"\n        if self.segmentation_model is None:\n            return np.zeros_like(image[:, :, 0], dtype=np.uint8)\n\n        # Preprocess image\n        preprocess = transforms.Compose([\n            transforms.ToPILImage() if not isinstance(image, (np.ndarray)) else lambda x: x,\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Convert BGR to RGB for model input\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        input_tensor = preprocess(image_rgb).unsqueeze(0).to(self.device)\n\n        # Perform inference\n        with torch.no_grad():\n            output = self.segmentation_model(input_tensor)['out']\n\n        # Process output to get segmentation mask\n        output_predictions = output.argmax(1).squeeze(0).cpu().numpy()\n\n        # Normalize to 0-255 range for visualization\n        mask = ((output_predictions / output_predictions.max()) * 255).astype(np.uint8)\n\n        return mask\n\n    def publish_detections(self, detections, header):\n        \"\"\"\n        Publish detection results\n        \"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            # Only publish high-confidence detections\n            if detection['confidence'] > 0.5:\n                detection_msg = Detection2D()\n                detection_msg.header = header\n\n                # Set bounding box\n                bbox = detection['bbox']\n                detection_msg.bbox.size_x = bbox[2] - bbox[0]\n                detection_msg.bbox.size_y = bbox[3] - bbox[1]\n                detection_msg.bbox.center.x = (bbox[0] + bbox[2]) / 2\n                detection_msg.bbox.center.y = (bbox[1] + bbox[3]) / 2\n\n                # Set hypothesis\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.hypothesis.class_id = str(detection['class_name'])\n                hypothesis.hypothesis.score = detection['confidence']\n\n                detection_msg.results.append(hypothesis)\n                detection_array.detections.append(detection_msg)\n\n        self.detection_pub.publish(detection_array)\n\n    def publish_segmentation(self, mask, header):\n        \"\"\"\n        Publish segmentation mask\n        \"\"\"\n        try:\n            mask_msg = self.bridge.cv2_to_imgmsg(mask, encoding='mono8')\n            mask_msg.header = header\n            self.segmentation_pub.publish(mask_msg)\n        except Exception as e:\n            self.get_logger().error(f'Error publishing segmentation: {e}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_pipeline = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-perception-configuration",children:"Isaac Perception Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Here's an example of how to configure Isaac perception parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# perception_config.yaml\nperception_pipeline:\n  ros__parameters:\n    # Processing parameters\n    detection_threshold: 0.5\n    segmentation_threshold: 0.7\n    max_objects: 10\n\n    # Performance parameters\n    input_width: 640\n    input_height: 480\n    max_processing_rate: 30.0\n\n    # Model optimization\n    tensorrt_precision: "FP16"  # or "FP32"\n    batch_size: 1\n\n    # Sensor fusion\n    enable_sensor_fusion: true\n    fusion_method: "probabilistic"\n\n    # Tracking\n    enable_object_tracking: true\n    tracking_algorithm: "deep_sort"\n    max_disappeared_frames: 30\n\ndetection_model:\n  ros__parameters:\n    model_path: "/models/yolov5s.pt"\n    model_type: "yolo"\n    input_format: "RGB"\n    confidence_threshold: 0.5\n    nms_threshold: 0.4\n\nsegmentation_model:\n  ros__parameters:\n    model_path: "/models/deeplabv3_resnet50.pth"\n    model_type: "deeplab"\n    input_format: "RGB"\n    num_classes: 21\n    output_format: "mask"\n\ncamera_preprocessing:\n  ros__parameters:\n    enable_distortion_correction: true\n    enable_color_conversion: true\n    target_encoding: "rgb8"\n    resize_width: 640\n    resize_height: 480\n    enable_normalization: true\n    normalization_mean: [0.485, 0.456, 0.406]\n    normalization_std: [0.229, 0.224, 0.225]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-practical-tutorial",children:"Step-by-Step Practical Tutorial"}),"\n",(0,t.jsx)(n.h3,{id:"building-an-isaac-perception-system",children:"Building an Isaac Perception System"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create a perception package"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python isaac_perception_system --dependencies rclpy std_msgs sensor_msgs geometry_msgs vision_msgs cv_bridge tf2_ros\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Navigate to the package directory"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd isaac_perception_system\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create the main module directory"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir isaac_perception_system\ntouch isaac_perception_system/__init__.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create the perception pipeline"})," (",(0,t.jsx)(n.code,{children:"isaac_perception_system/perception_pipeline.py"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Use the Isaac perception pipeline code example above\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create config directory"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir config\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create perception configuration"})," (",(0,t.jsx)(n.code,{children:"config/perception_config.yaml"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Use the configuration example above\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create launch directory"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir launch\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create a launch file"})," (",(0,t.jsx)(n.code,{children:"launch/isaac_perception_system.launch.py"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    # Get package share directory\n    pkg_share = get_package_share_directory('isaac_perception_system')\n    config_file = os.path.join(pkg_share, 'config', 'perception_config.yaml')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n\n        # Isaac perception pipeline\n        Node(\n            package='isaac_perception_system',\n            executable='isaac_perception_system.perception_pipeline',\n            name='isaac_perception_pipeline',\n            parameters=[\n                config_file,\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Update setup.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'isaac_perception_system'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\n        (os.path.join('share', package_name, 'config'), glob('config/*.yaml')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='User',\n    maintainer_email='user@example.com',\n    description='Isaac perception system with GPU acceleration',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'perception_pipeline = isaac_perception_system.perception_pipeline:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Build the package"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select isaac_perception_system\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Source the workspace"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch the perception system"})," (requires CUDA-enabled GPU):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_perception_system isaac_perception_system.launch.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Monitor the perception outputs"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# View detection results\nros2 topic echo /perception/detections\n\n# View segmentation masks\nros2 run image_view image_view _image:=/perception/segmentation_mask\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered Isaac perception systems, which leverage GPU acceleration to enable real-time computer vision and AI processing on robotic platforms. We explored the architecture of perception pipelines, configuration options, and practical implementation techniques."}),"\n",(0,t.jsx)(n.p,{children:"Isaac's perception capabilities enable robots to understand their environment with high accuracy and performance. By leveraging GPU acceleration, these systems can process complex visual information in real-time, making them suitable for demanding applications like autonomous navigation and object manipulation."}),"\n",(0,t.jsx)(n.h2,{id:"mini-quiz",children:"Mini-Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the main advantage of GPU acceleration in perception systems?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A) Lower cost"}),"\n",(0,t.jsx)(n.li,{children:"B) Faster processing of complex algorithms"}),"\n",(0,t.jsx)(n.li,{children:"C) Simpler code implementation"}),"\n",(0,t.jsx)(n.li,{children:"D) Reduced memory usage"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which Isaac package is used for object detection with TensorRT acceleration?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A) isaac_ros_segmentation"}),"\n",(0,t.jsx)(n.li,{children:"B) isaac_ros_detectnet"}),"\n",(0,t.jsx)(n.li,{children:"C) isaac_ros_pose_estimation"}),"\n",(0,t.jsx)(n.li,{children:"D) isaac_ros_visual_slam"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is semantic segmentation?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A) Detecting objects in images"}),"\n",(0,t.jsx)(n.li,{children:"B) Pixel-level classification of image content"}),"\n",(0,t.jsx)(n.li,{children:"C) Estimating object poses"}),"\n",(0,t.jsx)(n.li,{children:"D) Creating 3D maps"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which model optimization technique does Isaac use for inference acceleration?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A) TensorRT"}),"\n",(0,t.jsx)(n.li,{children:"B) OpenVINO"}),"\n",(0,t.jsx)(n.li,{children:"C) ONNX Runtime"}),"\n",(0,t.jsx)(n.li,{children:"D) TensorFlow Lite"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the purpose of sensor fusion in perception systems?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A) To reduce sensor costs"}),"\n",(0,t.jsx)(n.li,{children:"B) To combine data from multiple sensors for better accuracy"}),"\n",(0,t.jsx)(n.li,{children:"C) To simplify sensor installation"}),"\n",(0,t.jsx)(n.li,{children:"D) To increase sensor range"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answers"}),": 1-B, 2-B, 3-B, 4-A, 5-B"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);