"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[3906],{939:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>r,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4_vla/vla-planning-algorithms","title":"VLA Planning Algorithms","description":"Learning Objectives","source":"@site/docs/module4_vla/19-vla-planning-algorithms.md","sourceDirName":"module4_vla","slug":"/module4_vla/vla-planning-algorithms","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/vla-planning-algorithms","draft":false,"unlisted":false,"editUrl":"https://github.com/Iqrasajid-01/docs/module4_vla/19-vla-planning-algorithms.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"title":"VLA Planning Algorithms","sidebar_label":"19 - VLA Planning Algorithms"},"sidebar":"tutorialSidebar","previous":{"title":"18 - Vision-Language-Action Models","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/18-vision-language-action-models"},"next":{"title":"20 - VLA Integration with Robotics","permalink":"/Physical-AI-and-Humanoid-Robotics-Book/docs/module4_vla/integration-with-robotics"}}');var t=a(4848),s=a(8453);const l={title:"VLA Planning Algorithms",sidebar_label:"19 - VLA Planning Algorithms"},o="VLA Planning Algorithms",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"VLA Planning Architecture",id:"vla-planning-architecture",level:3},{value:"Planning Approaches",id:"planning-approaches",level:3},{value:"Language-Guided Planning",id:"language-guided-planning",level:3},{value:"Visual-Guided Planning",id:"visual-guided-planning",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Flow Diagram",id:"flow-diagram",level:2},{value:"Code Example: VLA Planning Algorithm",id:"code-example-vla-planning-algorithm",level:2},{value:"Advanced VLA Planning with Neural Networks",id:"advanced-vla-planning-with-neural-networks",level:2},{value:"Step-by-Step Practical Tutorial",id:"step-by-step-practical-tutorial",level:2},{value:"Implementing VLA Planning Algorithms",id:"implementing-vla-planning-algorithms",level:3},{value:"Summary",id:"summary",level:2},{value:"Mini-Quiz",id:"mini-quiz",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vla-planning-algorithms",children:"VLA Planning Algorithms"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand how VLA models integrate with robotic planning systems"}),"\n",(0,t.jsx)(e.li,{children:"Implement planning algorithms that incorporate vision and language inputs"}),"\n",(0,t.jsx)(e.li,{children:"Design hierarchical planning approaches for complex VLA tasks"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the effectiveness of different VLA planning strategies"}),"\n",(0,t.jsx)(e.li,{children:"Optimize planning algorithms for real-time robotic applications"}),"\n",(0,t.jsx)(e.li,{children:"Apply VLA planning to manipulation and navigation tasks"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"VLA (Vision-Language-Action) planning algorithms represent a sophisticated approach to robotic task planning that incorporates visual perception and natural language understanding into the planning process. Unlike traditional planning approaches that operate on symbolic representations, VLA planning algorithms work with raw sensory inputs and natural language commands to generate executable action sequences."}),"\n",(0,t.jsx)(e.p,{children:"The integration of vision and language into planning enables robots to understand complex, high-level instructions and execute them in dynamic, real-world environments. This chapter explores various approaches to VLA planning, from hierarchical task networks to neural planning architectures."}),"\n",(0,t.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(e.h3,{id:"vla-planning-architecture",children:"VLA Planning Architecture"}),"\n",(0,t.jsx)(e.p,{children:"VLA planning typically involves multiple levels:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Planning"}),": High-level task decomposition based on language commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Motion Planning"}),": Path planning considering visual obstacles and goals"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Sequencing"}),": Low-level action execution based on visual feedback"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Replanning"}),": Dynamic adaptation to environmental changes"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"planning-approaches",children:"Planning Approaches"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Symbolic Planning"}),": Traditional approaches adapted for VLA contexts"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Neural Planning"}),": End-to-end learning of planning policies"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hybrid Approaches"}),": Combining symbolic and neural methods"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning planning strategies through interaction"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"language-guided-planning",children:"Language-Guided Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Command Interpretation"}),": Converting natural language to planning goals"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Understanding"}),": Extracting object properties and spatial relations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Reasoning"}),": Understanding sequential and temporal aspects of commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Awareness"}),": Incorporating environmental context into planning"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"visual-guided-planning",children:"Visual-Guided Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene Understanding"}),": Interpreting visual input for planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Detection"}),": Identifying relevant objects for task execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Spatial Reasoning"}),": Understanding spatial relationships between objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Adaptation"}),": Adjusting plans based on visual feedback"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Flow Diagram",src:a(1194).A+"",width:"1653",height:"891"})}),"\n",(0,t.jsx)(e.h2,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Flow Diagram",src:a(5813).A+"",width:"1288",height:"849"})}),"\n",(0,t.jsx)(e.h2,{id:"code-example-vla-planning-algorithm",children:"Code Example: VLA Planning Algorithm"}),"\n",(0,t.jsx)(e.p,{children:"Here's an example implementation of a VLA planning algorithm:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom typing import List, Dict, Tuple, Any, Optional\nimport heapq\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass Action(Enum):\n    """Robot actions for VLA planning"""\n    MOVE_FORWARD = "move_forward"\n    MOVE_BACKWARD = "move_backward"\n    TURN_LEFT = "turn_left"\n    TURN_RIGHT = "turn_right"\n    GRASP = "grasp"\n    RELEASE = "release"\n    APPROACH_OBJECT = "approach_object"\n    AVOID_OBSTACLE = "avoid_obstacle"\n\n\n@dataclass\nclass PlanningState:\n    """State representation for VLA planning"""\n    position: np.ndarray  # Robot position [x, y, theta]\n    objects: Dict[str, np.ndarray]  # Object positions {name: [x, y]}\n    goal: Optional[str] = None  # Goal object or location\n    language_command: str = ""  # Original command\n    executed_actions: List[Action] = None\n\n    def __post_init__(self):\n        if self.executed_actions is None:\n            self.executed_actions = []\n\n\n@dataclass\nclass PlanStep:\n    """Represents a step in the plan"""\n    action: Action\n    cost: float\n    heuristic: float\n    state: PlanningState\n\n    def __lt__(self, other):\n        return (self.cost + self.heuristic) < (other.cost + other.heuristic)\n\n\nclass LanguageParser:\n    """\n    Parses natural language commands into structured goals\n    """\n    def __init__(self):\n        self.object_keywords = {\n            \'red\', \'blue\', \'green\', \'cup\', \'box\', \'ball\', \'table\', \'chair\'\n        }\n        self.action_keywords = {\n            \'pick\', \'grasp\', \'take\', \'move\', \'go\', \'approach\', \'place\', \'put\'\n        }\n        self.spatial_keywords = {\n            \'left\', \'right\', \'front\', \'back\', \'near\', \'far\', \'between\', \'next_to\'\n        }\n\n    def parse_command(self, command: str) -> Dict[str, Any]:\n        """\n        Parse natural language command into structured representation\n        """\n        command_lower = command.lower()\n        tokens = command_lower.split()\n\n        # Extract objects\n        objects = [token for token in tokens if token in self.object_keywords]\n\n        # Extract actions\n        actions = [token for token in tokens if token in self.action_keywords]\n\n        # Extract spatial relations\n        spatial = [token for token in tokens if token in self.spatial_keywords]\n\n        # Determine goal based on command\n        goal = self._determine_goal(command_lower, objects, actions)\n\n        return {\n            \'command\': command,\n            \'objects\': objects,\n            \'actions\': actions,\n            \'spatial\': spatial,\n            \'goal\': goal,\n            \'raw_tokens\': tokens\n        }\n\n    def _determine_goal(self, command: str, objects: List[str], actions: List[str]) -> str:\n        """\n        Determine the planning goal from the command\n        """\n        if \'pick\' in actions or \'grasp\' in actions or \'take\' in actions:\n            if objects:\n                return f"grasp_{objects[0]}"\n        elif \'move\' in actions or \'go\' in actions or \'approach\' in actions:\n            if objects:\n                return f"approach_{objects[0]}"\n        elif \'place\' in actions or \'put\' in actions:\n            if objects:\n                return f"place_{objects[0]}"\n\n        return "navigate"\n\n\nclass VisualSceneAnalyzer:\n    """\n    Analyzes visual input to extract scene information\n    """\n    def __init__(self):\n        # In a real implementation, this would use computer vision models\n        pass\n\n    def analyze_scene(self, visual_input: np.ndarray) -> Dict[str, np.ndarray]:\n        """\n        Analyze visual input to identify objects and their positions\n        """\n        # This is a simplified simulation\n        # In practice, this would use object detection and pose estimation\n        objects = {\n            \'red_cup\': np.array([1.0, 0.5]),\n            \'blue_box\': np.array([2.0, 1.0]),\n            \'green_ball\': np.array([0.5, 1.5]),\n            \'table\': np.array([1.5, 0.0])\n        }\n        return objects\n\n\nclass VLAPlanner:\n    """\n    VLA planning algorithm that integrates vision and language\n    """\n    def __init__(self, world_size: Tuple[float, float] = (5.0, 5.0)):\n        self.world_size = world_size\n        self.language_parser = LanguageParser()\n        self.visual_analyzer = VisualSceneAnalyzer()\n        self.grid_resolution = 0.1  # 10cm resolution\n\n    def plan(self,\n             initial_state: PlanningState,\n             language_command: str,\n             max_steps: int = 100) -> Optional[List[Action]]:\n        """\n        Plan a sequence of actions to achieve the goal specified by the language command\n        """\n        # Parse the language command\n        parsed_command = self.language_parser.parse_command(language_command)\n\n        # Update state with parsed information\n        initial_state.language_command = language_command\n        initial_state.goal = parsed_command[\'goal\']\n\n        # Use A* search for planning\n        open_set = []\n        closed_set = set()\n\n        # Calculate initial heuristic\n        initial_heuristic = self._calculate_heuristic(initial_state, parsed_command[\'goal\'])\n\n        initial_step = PlanStep(\n            action=None,\n            cost=0.0,\n            heuristic=initial_heuristic,\n            state=initial_state\n        )\n\n        heapq.heappush(open_set, initial_step)\n\n        step_count = 0\n        while open_set and step_count < max_steps:\n            current_step = heapq.heappop(open_set)\n\n            # Check if goal is reached\n            if self._is_goal_reached(current_step.state, parsed_command[\'goal\']):\n                return self._reconstruct_plan(current_step)\n\n            closed_set.add(self._state_key(current_step.state))\n\n            # Generate possible next states\n            for action in self._get_possible_actions(current_step.state):\n                new_state = self._apply_action(current_step.state, action)\n                state_key = self._state_key(new_state)\n\n                if state_key in closed_set:\n                    continue\n\n                new_cost = current_step.cost + self._action_cost(action)\n                new_heuristic = self._calculate_heuristic(new_state, parsed_command[\'goal\'])\n\n                new_step = PlanStep(\n                    action=action,\n                    cost=new_cost,\n                    heuristic=new_heuristic,\n                    state=new_state\n                )\n\n                heapq.heappush(open_set, new_step)\n\n            step_count += 1\n\n        return None  # No plan found\n\n    def _get_possible_actions(self, state: PlanningState) -> List[Action]:\n        """\n        Get possible actions from the current state\n        """\n        # For now, return all possible actions\n        # In practice, this would depend on the current context\n        return [\n            Action.MOVE_FORWARD,\n            Action.MOVE_BACKWARD,\n            Action.TURN_LEFT,\n            Action.TURN_RIGHT,\n            Action.GRASP,\n            Action.RELEASE,\n            Action.APPROACH_OBJECT,\n            Action.AVOID_OBSTACLE\n        ]\n\n    def _apply_action(self, state: PlanningState, action: Action) -> PlanningState:\n        """\n        Apply an action to the current state to get a new state\n        """\n        new_state = PlanningState(\n            position=state.position.copy(),\n            objects={k: v.copy() for k, v in state.objects.items()},\n            goal=state.goal,\n            language_command=state.language_command,\n            executed_actions=state.executed_actions + [action]\n        )\n\n        # Update position based on action\n        if action == Action.MOVE_FORWARD:\n            # Move forward 10cm\n            new_state.position[0] += 0.1 * np.cos(new_state.position[2])\n            new_state.position[1] += 0.1 * np.sin(new_state.position[2])\n        elif action == Action.MOVE_BACKWARD:\n            # Move backward 10cm\n            new_state.position[0] -= 0.1 * np.cos(new_state.position[2])\n            new_state.position[1] -= 0.1 * np.sin(new_state.position[2])\n        elif action == Action.TURN_LEFT:\n            # Turn left 15 degrees\n            new_state.position[2] += np.radians(15)\n        elif action == Action.TURN_RIGHT:\n            # Turn right 15 degrees\n            new_state.position[2] -= np.radians(15)\n        elif action == Action.APPROACH_OBJECT:\n            # Find the closest object and move toward it\n            if state.goal and state.goal.startswith("approach_"):\n                obj_name = state.goal.replace("approach_", "")\n                if obj_name in new_state.objects:\n                    obj_pos = new_state.objects[obj_name]\n                    robot_pos = new_state.position[:2]\n\n                    # Move halfway toward the object\n                    direction = obj_pos - robot_pos\n                    distance = np.linalg.norm(direction)\n                    if distance > 0.1:  # If not already close\n                        new_pos = robot_pos + 0.5 * direction * min(0.1, distance) / distance\n                        new_state.position[0] = new_pos[0]\n                        new_state.position[1] = new_pos[1]\n\n        return new_state\n\n    def _action_cost(self, action: Action) -> float:\n        """\n        Return the cost of performing an action\n        """\n        cost_map = {\n            Action.MOVE_FORWARD: 1.0,\n            Action.MOVE_BACKWARD: 1.2,  # Slightly more expensive\n            Action.TURN_LEFT: 0.8,\n            Action.TURN_RIGHT: 0.8,\n            Action.GRASP: 2.0,\n            Action.RELEASE: 1.5,\n            Action.APPROACH_OBJECT: 1.0,\n            Action.AVOID_OBSTACLE: 1.5\n        }\n        return cost_map.get(action, 1.0)\n\n    def _calculate_heuristic(self, state: PlanningState, goal: str) -> float:\n        """\n        Calculate heuristic distance to goal\n        """\n        if not goal:\n            return 0.0\n\n        if goal.startswith("approach_") or goal.startswith("grasp_"):\n            obj_name = goal.replace("approach_", "").replace("grasp_", "")\n            if obj_name in state.objects:\n                obj_pos = state.objects[obj_name]\n                robot_pos = state.position[:2]\n                distance = np.linalg.norm(obj_pos - robot_pos)\n                return distance\n\n        # Default heuristic\n        return 1.0\n\n    def _is_goal_reached(self, state: PlanningState, goal: str) -> bool:\n        """\n        Check if the goal has been reached\n        """\n        if not goal:\n            return True\n\n        if goal.startswith("approach_"):\n            obj_name = goal.replace("approach_", "")\n            if obj_name in state.objects:\n                obj_pos = state.objects[obj_name]\n                robot_pos = state.position[:2]\n                distance = np.linalg.norm(obj_pos - robot_pos)\n                return distance < 0.2  # Within 20cm\n\n        elif goal.startswith("grasp_"):\n            # This is simplified - in reality, grasping involves more complex conditions\n            obj_name = goal.replace("grasp_", "")\n            if obj_name in state.objects:\n                obj_pos = state.objects[obj_name]\n                robot_pos = state.position[:2]\n                distance = np.linalg.norm(obj_pos - robot_pos)\n                return distance < 0.1  # Within 10cm for grasping\n\n        return False\n\n    def _state_key(self, state: PlanningState) -> str:\n        """\n        Create a unique key for the state (for duplicate detection)\n        """\n        pos_str = f"{state.position[0]:.2f}_{state.position[1]:.2f}_{state.position[2]:.2f}"\n        obj_str = "_".join([f"{name}_{pos[0]:.2f}_{pos[1]:.2f}" for name, pos in state.objects.items()])\n        return f"{pos_str}_{obj_str}_{state.goal}"\n\n    def _reconstruct_plan(self, final_step: PlanStep) -> List[Action]:\n        """\n        Reconstruct the plan from the final step\n        """\n        # For this simple implementation, we return the executed actions\n        # In a real A* implementation, we would backtrack through parent pointers\n        return [action for action in final_step.state.executed_actions if action is not None]\n\n\nclass HierarchicalVLAPlanner:\n    """\n    Hierarchical VLA planner that breaks down complex tasks\n    """\n    def __init__(self):\n        self.low_level_planner = VLAPlanner()\n        self.language_parser = LanguageParser()\n\n    def plan_hierarchical(self,\n                         initial_state: PlanningState,\n                         language_command: str) -> Optional[List[Action]]:\n        """\n        Plan using hierarchical approach\n        """\n        # Parse the high-level command\n        parsed = self.language_parser.parse_command(language_command)\n\n        # Break down into subtasks\n        subtasks = self._decompose_task(parsed, initial_state)\n\n        # Plan each subtask\n        full_plan = []\n        current_state = initial_state\n\n        for subtask in subtasks:\n            subtask_plan = self.low_level_planner.plan(current_state, subtask)\n            if subtask_plan is None:\n                return None  # Failed to plan for subtask\n\n            full_plan.extend(subtask_plan)\n\n            # Update state with execution of subtask plan\n            for action in subtask_plan:\n                current_state = self.low_level_planner._apply_action(current_state, action)\n\n        return full_plan\n\n    def _decompose_task(self, parsed_command: Dict, state: PlanningState) -> List[str]:\n        """\n        Decompose a high-level command into subtasks\n        """\n        subtasks = []\n\n        if \'grasp\' in parsed_command[\'actions\'] or \'pick\' in parsed_command[\'actions\']:\n            # Need to approach object first, then grasp\n            if parsed_command[\'objects\']:\n                obj = parsed_command[\'objects\'][0]\n                subtasks.append(f"approach {obj}")\n                subtasks.append(f"grasp {obj}")\n\n        elif \'move\' in parsed_command[\'actions\'] or \'go\' in parsed_command[\'actions\']:\n            # Direct navigation\n            if parsed_command[\'objects\']:\n                obj = parsed_command[\'objects\'][0]\n                subtasks.append(f"approach {obj}")\n\n        else:\n            # Default: try to interpret as navigation\n            subtasks.append(parsed_command[\'command\'])\n\n        return subtasks\n\n\ndef main():\n    """\n    Example usage of VLA planning\n    """\n    print("VLA Planning Example")\n\n    # Initialize planner\n    planner = VLAPlanner()\n\n    # Create initial state\n    initial_state = PlanningState(\n        position=np.array([0.0, 0.0, 0.0]),  # x, y, theta\n        objects={\n            \'red_cup\': np.array([1.0, 0.5]),\n            \'blue_box\': np.array([2.0, 1.0]),\n            \'green_ball\': np.array([0.5, 1.5]),\n            \'table\': np.array([1.5, 0.0])\n        }\n    )\n\n    # Example command\n    command = "approach the red cup"\n\n    print(f"Planning for command: \'{command}\'")\n\n    # Plan using VLA planner\n    plan = planner.plan(initial_state, command)\n\n    if plan:\n        print(f"Plan found with {len(plan)} actions:")\n        for i, action in enumerate(plan):\n            print(f"  {i+1}. {action.value}")\n    else:\n        print("No plan found")\n\n    # Example with hierarchical planner\n    print("\\nUsing hierarchical planner:")\n    hplanner = HierarchicalVLAPlanner()\n    hplan = hplanner.plan_hierarchical(initial_state, "pick up the red cup")\n\n    if hplan:\n        print(f"Hierarchical plan found with {len(hplan)} actions:")\n        for i, action in enumerate(hplan):\n            print(f"  {i+1}. {action.value}")\n    else:\n        print("No hierarchical plan found")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-vla-planning-with-neural-networks",children:"Advanced VLA Planning with Neural Networks"}),"\n",(0,t.jsx)(e.p,{children:"Here's an example of a neural network-based VLA planning approach:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import List, Tuple\n\n\nclass VLAPlanningNetwork(nn.Module):\n    """\n    Neural network for VLA planning\n    """\n    def __init__(self,\n                 vision_dim: int = 512,\n                 language_dim: int = 512,\n                 state_dim: int = 10,\n                 action_dim: int = 8,\n                 hidden_dim: int = 256):\n        super().__init__()\n\n        self.vision_dim = vision_dim\n        self.language_dim = language_dim\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.hidden_dim = hidden_dim\n\n        # Vision encoder (simplified)\n        self.vision_encoder = nn.Sequential(\n            nn.Linear(vision_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Language encoder (simplified)\n        self.language_encoder = nn.Sequential(\n            nn.Linear(language_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # State encoder\n        self.state_encoder = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Multimodal fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 3, hidden_dim * 2),  # vision + language + state\n            nn.ReLU(),\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Policy network for action selection\n        self.policy = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, action_dim)\n        )\n\n        # Value network for planning\n        self.value = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self,\n                vision_features: torch.Tensor,\n                language_features: torch.Tensor,\n                state_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Forward pass of the VLA planning network\n        """\n        # Encode each modality\n        vision_out = self.vision_encoder(vision_features)\n        language_out = self.language_encoder(language_features)\n        state_out = self.state_encoder(state_features)\n\n        # Fuse modalities\n        fused = torch.cat([vision_out, language_out, state_out], dim=-1)\n        fused = self.fusion(fused)\n\n        # Get action logits and value\n        action_logits = self.policy(fused)\n        value = self.value(fused)\n\n        return action_logits, value\n\n    def plan_step(self,\n                  vision_features: torch.Tensor,\n                  language_features: torch.Tensor,\n                  state_features: torch.Tensor,\n                  temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Plan a single step\n        """\n        action_logits, value = self.forward(vision_features, language_features, state_features)\n\n        # Apply temperature scaling\n        action_probs = F.softmax(action_logits / temperature, dim=-1)\n\n        # Sample action\n        action_dist = torch.distributions.Categorical(action_probs)\n        action = action_dist.sample()\n\n        return action, value\n\n\nclass NeuralVLAPlanner:\n    """\n    Neural network-based VLA planner\n    """\n    def __init__(self, model_path: str = None):\n        self.model = VLAPlanningNetwork()\n\n        if model_path:\n            # Load pre-trained model\n            self.model.load_state_dict(torch.load(model_path))\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.model.to(self.device)\n\n    def plan_with_learning(self,\n                          initial_state_features: torch.Tensor,\n                          language_features: torch.Tensor,\n                          vision_features: torch.Tensor,\n                          max_steps: int = 20) -> List[int]:\n        """\n        Plan using the neural network with potential learning\n        """\n        plan = []\n        current_state = initial_state_features.clone()\n\n        for step in range(max_steps):\n            # Get action and value\n            action, value = self.model.plan_step(\n                vision_features.unsqueeze(0),  # Add batch dimension\n                language_features.unsqueeze(0),\n                current_state.unsqueeze(0)\n            )\n\n            action_idx = action.item()\n            plan.append(action_idx)\n\n            # In a real implementation, you would update the state based on the action\n            # For this example, we\'ll just continue with the same state\n            # current_state = self._update_state(current_state, action_idx)\n\n            # Check if goal is reached (simplified)\n            if action_idx == 7:  # Assuming action 7 is "done"\n                break\n\n        return plan\n\n    def train_step(self,\n                   vision_batch: torch.Tensor,\n                   language_batch: torch.Tensor,\n                   state_batch: torch.Tensor,\n                   action_batch: torch.Tensor,\n                   target_values: torch.Tensor) -> float:\n        """\n        Train the model on a batch of data\n        """\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        action_logits, values = self.model(vision_batch, language_batch, state_batch)\n\n        # Compute loss: policy loss + value loss\n        policy_loss = F.cross_entropy(action_logits, action_batch)\n        value_loss = F.mse_loss(values.squeeze(), target_values)\n        total_loss = policy_loss + 0.5 * value_loss\n\n        total_loss.backward()\n        self.optimizer.step()\n\n        return total_loss.item()\n\n\ndef create_synthetic_dataset(num_samples: int = 1000) -> Tuple[torch.Tensor, ...]:\n    """\n    Create a synthetic dataset for training VLA planning networks\n    """\n    vision_dim, language_dim, state_dim, action_dim = 512, 512, 10, 8\n\n    vision_features = torch.randn(num_samples, vision_dim)\n    language_features = torch.randn(num_samples, language_dim)\n    state_features = torch.randn(num_samples, state_dim)\n    actions = torch.randint(0, action_dim, (num_samples,))\n    values = torch.randn(num_samples)  # Target values for training\n\n    return vision_features, language_features, state_features, actions, values\n\n\ndef train_neural_vla_planner():\n    """\n    Example training loop for neural VLA planner\n    """\n    # Create dataset\n    vision_data, language_data, state_data, actions, values = create_synthetic_dataset(1000)\n\n    # Initialize planner\n    planner = NeuralVLAPlanner()\n\n    # Training loop\n    batch_size = 32\n    num_epochs = 10\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        num_batches = len(vision_data) // batch_size\n\n        for i in range(num_batches):\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n\n            batch_vision = vision_data[start_idx:end_idx].to(planner.device)\n            batch_language = language_data[start_idx:end_idx].to(planner.device)\n            batch_state = state_data[start_idx:end_idx].to(planner.device)\n            batch_actions = actions[start_idx:end_idx].to(planner.device)\n            batch_values = values[start_idx:end_idx].to(planner.device)\n\n            loss = planner.train_step(\n                batch_vision, batch_language, batch_state,\n                batch_actions, batch_values\n            )\n\n            epoch_loss += loss\n\n        avg_loss = epoch_loss / num_batches\n        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")\n\n    return planner\n\n\ndef main_neural():\n    """\n    Example usage of neural VLA planning\n    """\n    print("Neural VLA Planning Example")\n\n    # Initialize planner\n    planner = NeuralVLAPlanner()\n\n    # Example planning\n    vision_features = torch.randn(512).to(planner.device)\n    language_features = torch.randn(512).to(planner.device)\n    state_features = torch.randn(10).to(planner.device)\n\n    plan = planner.plan_with_learning(\n        state_features, language_features, vision_features\n    )\n\n    print(f"Neural plan: {plan}")\n\n    # Example training (uncomment to run)\n    # trained_planner = train_neural_vla_planner()\n    # print("Training completed")\n\n\nif __name__ == "__main__":\n    main_neural()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"step-by-step-practical-tutorial",children:"Step-by-Step Practical Tutorial"}),"\n",(0,t.jsx)(e.h3,{id:"implementing-vla-planning-algorithms",children:"Implementing VLA Planning Algorithms"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Install required dependencies"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"pip3 install torch torchvision transformers numpy matplotlib\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create a VLA planning package"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python vla_planning_examples --dependencies rclpy std_msgs sensor_msgs geometry_msgs\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Navigate to the package directory"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"cd vla_planning_examples\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create the main module directory"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"mkdir vla_planning_examples\ntouch vla_planning_examples/__init__.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create the VLA planning implementation"})," (",(0,t.jsx)(e.code,{children:"vla_planning_examples/vla_planning.py"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Use the VLA planning algorithm code examples above\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create a ROS2 node for VLA planning"})," (",(0,t.jsx)(e.code,{children:"vla_planning_examples/vla_planning_node.py"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nfrom typing import Optional\n\n\nclass VLAPLanningNode(Node):\n    \"\"\"\n    ROS2 node for VLA planning\n    \"\"\"\n    def __init__(self):\n        super().__init__('vla_planning_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Initialize planning components\n        self.current_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.object_positions = {}\n        self.current_command = \"\"\n        self.plan = []\n        self.plan_index = 0\n\n        # Initialize planners\n        self.symbolic_planner = None  # Will be initialized\n        self.neural_planner = None    # Will be initialized\n\n        self.get_logger().info('VLA Planning Node initialized')\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, '/vla/command', self.command_callback, 10)\n\n        # Create publishers\n        self.action_pub = self.create_publisher(\n            Twist, '/cmd_vel', 10)\n        self.status_pub = self.create_publisher(\n            String, '/vla_planning/status', 10)\n\n        # Planning timer\n        self.plan_timer = self.create_timer(0.1, self.execute_plan)\n\n    def image_callback(self, msg):\n        \"\"\"\n        Handle incoming images for scene analysis\n        \"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.get_logger().debug(f'Received image for analysis: {cv_image.shape}')\n\n            # In a real implementation, analyze the scene\n            # self.analyze_scene(cv_image)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def odom_callback(self, msg):\n        \"\"\"\n        Handle odometry updates\n        \"\"\"\n        try:\n            self.current_pose[0] = msg.pose.pose.position.x\n            self.current_pose[1] = msg.pose.pose.position.y\n\n            # Convert quaternion to euler for theta\n            from tf_transformations import euler_from_quaternion\n            orientation = msg.pose.pose.orientation\n            _, _, self.current_pose[2] = euler_from_quaternion([\n                orientation.x, orientation.y, orientation.z, orientation.w\n            ])\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing odometry: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"\n        Handle natural language commands and plan accordingly\n        \"\"\"\n        try:\n            command = msg.data\n            self.get_logger().info(f'Received command: {command}')\n            self.current_command = command\n\n            # Plan based on command\n            self.plan = self._plan_for_command(command)\n\n            # Reset plan index\n            self.plan_index = 0\n\n            if self.plan:\n                self.get_logger().info(f'Generated plan with {len(self.plan)} steps')\n                status_msg = String()\n                status_msg.data = f'Planned {len(self.plan)} steps for: {command}'\n                self.status_pub.publish(status_msg)\n            else:\n                self.get_logger().warn('No plan generated for command')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n\n    def _plan_for_command(self, command: str) -> list:\n        \"\"\"\n        Plan actions for the given command\n        \"\"\"\n        # This is a simplified example\n        # In a real implementation, you would use the VLA planning algorithms\n        if 'approach' in command.lower() or 'go to' in command.lower():\n            return [Twist() for _ in range(10)]  # Placeholder actions\n        elif 'pick' in command.lower() or 'grasp' in command.lower():\n            return [Twist() for _ in range(15)]  # Placeholder actions\n        else:\n            return [Twist() for _ in range(5)]   # Default plan\n\n    def execute_plan(self):\n        \"\"\"\n        Execute the current plan step by step\n        \"\"\"\n        if not self.plan or self.plan_index >= len(self.plan):\n            return\n\n        # Get current action from plan\n        current_action = self.plan[self.plan_index]\n\n        # Publish action\n        self.action_pub.publish(current_action)\n\n        # Check if action is complete (simplified)\n        self.plan_index += 1\n\n        if self.plan_index >= len(self.plan):\n            self.get_logger().info('Plan completed')\n            status_msg = String()\n            status_msg.data = 'Plan completed'\n            self.status_pub.publish(status_msg)\n\n    def destroy_node(self):\n        \"\"\"\n        Clean up resources when node is destroyed\n        \"\"\"\n        self.get_logger().info('Cleaning up VLA Planning Node')\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_planning_node = VLAPLanningNode()\n\n    try:\n        rclpy.spin(vla_planning_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_planning_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create launch directory"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"mkdir launch\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create a launch file"})," (",(0,t.jsx)(e.code,{children:"launch/vla_planning_example.launch.py"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n\n        # VLA planning node\n        Node(\n            package='vla_planning_examples',\n            executable='vla_planning_examples.vla_planning_node',\n            name='vla_planning_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        )\n    ])\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Update setup.py"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'vla_planning_examples'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='User',\n    maintainer_email='user@example.com',\n    description='VLA planning examples for robotics',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'vla_planning_node = vla_planning_examples.vla_planning_node:main',\n        ],\n    },\n)\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Build the package"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select vla_planning_examples\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Source the workspace"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Run the VLA planning example"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"ros2 launch vla_planning_examples vla_planning_example.launch.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Test with sample commands"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# In another terminal\nros2 topic pub /vla/command std_msgs/String \"data: 'Approach the red cup'\"\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter covered VLA (Vision-Language-Action) planning algorithms, which integrate visual perception and natural language understanding into robotic planning systems. We explored both symbolic and neural approaches to VLA planning, implementation techniques, and practical applications."}),"\n",(0,t.jsx)(e.p,{children:"VLA planning enables robots to understand complex, high-level instructions and execute them in dynamic environments by combining multiple sensory modalities with sophisticated planning algorithms. The integration of vision and language into planning represents a significant advancement in robotic autonomy."}),"\n",(0,t.jsx)(e.h2,{id:"mini-quiz",children:"Mini-Quiz"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"What are the main components of VLA planning?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A) Vision and Language only"}),"\n",(0,t.jsx)(e.li,{children:"B) Vision, Language, and Action planning components"}),"\n",(0,t.jsx)(e.li,{children:"C) Perception and Control only"}),"\n",(0,t.jsx)(e.li,{children:"D) Navigation and Manipulation only"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Which planning approach combines symbolic and neural methods?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A) Pure neural planning"}),"\n",(0,t.jsx)(e.li,{children:"B) Pure symbolic planning"}),"\n",(0,t.jsx)(e.li,{children:"C) Hybrid approaches"}),"\n",(0,t.jsx)(e.li,{children:"D) Rule-based planning"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"What is the purpose of hierarchical VLA planning?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A) To reduce computational requirements"}),"\n",(0,t.jsx)(e.li,{children:"B) To break down complex tasks into manageable subtasks"}),"\n",(0,t.jsx)(e.li,{children:"C) To increase planning speed only"}),"\n",(0,t.jsx)(e.li,{children:"D) To reduce sensor requirements"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"In neural VLA planning, what does the value network estimate?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A) Action probabilities only"}),"\n",(0,t.jsx)(e.li,{children:"B) The expected return or value of a state"}),"\n",(0,t.jsx)(e.li,{children:"C) Object positions"}),"\n",(0,t.jsx)(e.li,{children:"D) Language embeddings"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"What type of reasoning is important for language-guided planning?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A) Spatial reasoning only"}),"\n",(0,t.jsx)(e.li,{children:"B) Temporal reasoning only"}),"\n",(0,t.jsx)(e.li,{children:"C) Both spatial and temporal reasoning"}),"\n",(0,t.jsx)(e.li,{children:"D) Geometric reasoning only"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Answers"}),": 1-B, 2-C, 3-B, 4-B, 5-C"]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},1194:(n,e,a)=>{a.d(e,{A:()=>i});const i=a.p+"assets/images/ch19-ad-e5595a053b24259721876d01008f09e6.svg"},5813:(n,e,a)=>{a.d(e,{A:()=>i});const i=a.p+"assets/images/ch19-flow-b718a1431fd56a5802eaf872958ce66a.svg"},8453:(n,e,a)=>{a.d(e,{R:()=>l,x:()=>o});var i=a(6540);const t={},s=i.createContext(t);function l(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);